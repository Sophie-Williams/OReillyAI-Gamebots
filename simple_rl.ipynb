{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import gym\n",
    "import universe\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main reinforcement learning step\n",
    "def determine_turn(turn, observation_n, j, total_sum, prev_total_sum, reward_n):\n",
    "\t# For every 15 iterations, sum over thetotal observations, and then take the mean.\n",
    "\t# If the mean is lower than 0, change the direction of the car.\n",
    "\t# If we go 15+ iterations and get a reward each step, we're doing something right\n",
    "\t# thats when we turn\n",
    "\tif(j >= 15):\n",
    "\t\tif(total_sum/ j ) == 0:\n",
    "\t\t\tturn = True\n",
    "\t\telse:\n",
    "\t\t\tturn = False\n",
    "\n",
    "\t\t#reset vars\n",
    "\t\ttotal_sum = 0\n",
    "\t\tj = 0\n",
    "\t\tprev_total_sum = total_sum\n",
    "\t\ttotal_sum = 0\n",
    "\n",
    "\telse:\n",
    "\t\tturn = False\n",
    "\tif(observation_n != None):\n",
    "\t\t#increment counter and reward sum\n",
    "\t\tj+=1\n",
    "\t\ttotal_sum += reward_n\n",
    "\treturn(turn, j, total_sum, prev_total_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "\t# Initialize the environment\n",
    "\tenv = gym.make('flashgames.DuskDrive-v0')\n",
    "\tobservation_n = env.reset()\n",
    "\n",
    "\t# Initialize some variables\n",
    "\t# Define the number of game iterations\n",
    "\tn = 0\n",
    "\tj = 0\n",
    "\t# sum of total observations\n",
    "\ttotal_sum = 0\n",
    "\tprev_total_sum = 0\n",
    "\tturn = False\n",
    "\n",
    "\t#define our turns or keyboard actions\n",
    "\tleft = [('KeyEvent', 'ArrowUp', True) ,('KeyEvent', 'ArrowLeft', True), ('KeyEvent', 'ArrowRight', False)]\n",
    "\tright = [('KeyEvent', 'ArrowUp', True) ,('KeyEvent', 'ArrowLeft', False), ('KeyEvent', 'ArrowRight', True)]\n",
    "\tforward = [('KeyEvent', 'ArrowUp', True) ,('KeyEvent', 'ArrowLeft', False), ('KeyEvent', 'ArrowRight', False)]\n",
    "\n",
    "\n",
    "\t# Main logic\n",
    "\twhile True:\n",
    "\t\t#increment a counter for number of iterations\n",
    "\t\tn+=1\n",
    "\n",
    "\t\t#if at least one iteration is made, check if turn is needed\n",
    "\t\tif(n > 1):\n",
    "\t\t\t#if at least one iteration, check if a turn\n",
    "\t\t\tif(observation_n[0] != None):\n",
    "\t\t\t\t#store the reward in the previous score\n",
    "\t\t\t\tprev_score = reward_n[0]\n",
    "\n",
    "\t\t\t\t#should we turn?\n",
    "\t\t\t\tif(turn):\n",
    "\t\t\t\t\t#pick a random event\n",
    "\t\t\t\t\t#where to turn? \n",
    "\t\t\t\t\tevent = random.choice([left,right])\n",
    "\t\t\t\t\t#perform an action\n",
    "\t\t\t\t\taction_n = [event for ob in observation_n]\n",
    "\t\t\t\t\t#set turn to false\n",
    "\t\t\t\t\tturn = False\n",
    "\n",
    "\t\telif(~turn):\n",
    "\t\t\t# If no turn is needed, keep going straight\n",
    "\t\t\taction_n = [forward for ob in observation_n]\n",
    "\n",
    "\n",
    "\t\t# If there is an obseravtion, game has started, check if turn needed\n",
    "\t\tif(observation_n[0] != None):\n",
    "\t\t\tturn, j, total_sum, prev_total_sum = determine_turn(turn, observation_n[0], j, total_sum, prev_total_sum, reward_n[0])\n",
    "\n",
    "\t\t# Save new variables for each iteration.\n",
    "\t\tobservation_n, reward_n, done_n, info = env.step(action_n)\n",
    "\n",
    "\t\tenv.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-28 03:07:51,213] Making new env: flashgames.DuskDrive-v0\n",
      "[2017-06-28 03:07:51,261] Called reset on <Timer<Render<ThrottleVNCEnv<flashgames.DuskDrive-v0>>>> before configuring. Configuring automatically with default arguments\n",
      "[2017-06-28 03:07:51,264] Writing logs to file: /tmp/universe-36237.log\n",
      "[2017-06-28 03:07:57,401] Ports used: []\n",
      "[2017-06-28 03:07:57,402] [0] Creating container: image=quay.io/openai/universe.flashgames:0.20.28. Run the same thing by hand as: docker run -p 5900:5900 -p 15900:15900 --cap-add SYS_ADMIN --ipc host --privileged quay.io/openai/universe.flashgames:0.20.28\n",
      "[2017-06-28 03:07:58,196] [0] Could not start container: 500 Server Error: Internal Server Error (\"{\"message\":\"driver failed programming external connectivity on endpoint universe-FwMjDj-0 (381cc997783836285e8da6cd00d30db45ebea118ebd9ec068ba325b7c8a3c3b7): Error starting userland proxy: Bind for 0.0.0.0:5900 failed: port is already allocated\"}\")\n",
      "[2017-06-28 03:07:58,198] Killing and removing container: id=b4f9963d2daf9b63601d87ee7184672dd2fd007f5f95fd5b266445e02264d407\n",
      "[2017-06-28 03:08:03,031] Ports used: []\n",
      "[2017-06-28 03:08:03,033] [0] Creating container: image=quay.io/openai/universe.flashgames:0.20.28. Run the same thing by hand as: docker run -p 5901:5900 -p 15901:15900 --cap-add SYS_ADMIN --ipc host --privileged quay.io/openai/universe.flashgames:0.20.28\n",
      "[2017-06-28 03:08:04,341] Remote closed: address=localhost:5901\n",
      "[2017-06-28 03:08:04,347] At least one sockets was closed by the remote. Sleeping 1s...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m Setting VNC and rewarder password: openai\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [Wed Jun 28 07:08:04 UTC 2017] Waiting for /tmp/.X11-unix/X0 to be created (try 1/10)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Xvnc TigerVNC 1.7.0 - built Sep  8 2016 10:39:22\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Copyright (C) 1999-2016 TigerVNC Team and many others (see README.txt)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] See http://www.tigervnc.org for information on TigerVNC.\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Underlying X server release 11400000, The X.Org Foundation\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension VNC-EXTENSION\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension Generic Event Extension\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension SHAPE\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension MIT-SHM\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension XInputExtension\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension XTEST\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension BIG-REQUESTS\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension SYNC\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension XKEYBOARD\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension XC-MISC\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension XINERAMA\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension XFIXES\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension RENDER\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension RANDR\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension COMPOSITE\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension DAMAGE\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension MIT-SCREEN-SAVER\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension DOUBLE-BUFFER\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension RECORD\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension DPMS\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension X-Resource\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension XVideo\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension XVideo-MotionCompensation\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Initializing built-in extension GLX\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Wed Jun 28 07:08:04 2017\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  vncext:      VNC extension running!\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  vncext:      Listening for VNC connections on all interface(s), port 5900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  vncext:      created VNC server for screen 0\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] [dix] Could not init font path element /usr/share/fonts/X11/Type1/, removing from list!\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] [dix] Could not init font path element /usr/share/fonts/X11/75dpi/, removing from list!\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] [dix] Could not init font path element /usr/share/fonts/X11/100dpi/, removing from list!\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [Wed Jun 28 07:08:04 UTC 2017] [/usr/local/bin/sudoable-env-setup] Disabling outbound network traffic for none\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:08:04,889] Launching system_diagnostics_logger.py, recorder_logdir=/tmp/demo\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:08:04,921] Launching reward_recorder.py, recorder_logdir=/tmp/demo\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:08:04,941] Launching vnc_recorder.py, recorder_logdir=/tmp/demo\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:08:04,965] PID 54 launched with command ['sudo', '-H', '-u', 'nobody', 'DISPLAY=:0', 'DBUS_SESSION_BUS_ADDRESS=/dev/null', '/app/universe-envs/controlplane/bin/controlplane.py', '--rewarder-port=15901']\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:08:05,019] init detected end of child process 57 with exit code 0, not killed by signal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-28 03:08:05,355] Remote closed: address=localhost:5901\n",
      "[2017-06-28 03:08:05,370] Remote closed: address=localhost:15901\n",
      "[2017-06-28 03:08:05,377] At least one sockets was closed by the remote. Sleeping 1s...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:05 [error] 64#64: *1 connect() failed (111: Connection refused) while connecting to upstream, client: 172.17.0.1, server: , request: \"GET / HTTP/1.1\", upstream: \"http://127.0.0.1:15901/\", host: \"127.0.0.1:10003\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Wed Jun 28 07:08:05 2017\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 172.17.0.1 - openai [28/Jun/2017:07:08:05 +0000] \"GET / HTTP/1.1\" 502 182 \"-\" \"-\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  Connections: accepted: 172.17.0.1::35294\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:08:05,593] init detected end of child process 16 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m WebSocket server settings:\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m   - Listen on :5898\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m   - Flash security policy server\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m   - No SSL/TLS support (no cert file)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m   - proxying from :5898 to localhost:5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-28 03:08:06,385] Using the golang VNC implementation\n",
      "[2017-06-28 03:08:06,387] Using VNCSession arguments: {'subsample_level': 2, 'start_timeout': 7, 'fine_quality_level': 50, 'encoding': 'tight'}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "[2017-06-28 03:08:06,397] [0] Connecting to environment: vnc://localhost:5901 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://localhost:15901/viewer/?password=openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:06 [info] 64#64: *1 client 172.17.0.1 closed keepalive connection\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-28 03:08:07,300] [0:localhost:5901] Waiting on rewarder: failed to complete WebSocket handshake. Retry in 1s (slept 0s/7s): connection was closed uncleanly (WebSocket connection upgrade failed (502 - BadGateway))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Wed Jun 28 07:08:06 2017\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  Connections: closed: 172.17.0.1::35294 (Clean disconnection)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager: Framebuffer updates: 0\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:   Total: 0 rects, 0 pixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:          0 B (1:-nan ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  Connections: accepted: 172.17.0.1::35300\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  SConnection: Client needs protocol version 3.8\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  SConnection: Client requests security type VncAuth(2)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  VNCSConnST:  Server default pixel format depth 24 (32bpp) little-endian rgb888\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  VNCSConnST:  Client pixel format depth 24 (32bpp) little-endian bgr888\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:07 [error] 64#64: *3 connect() failed (111: Connection refused) while connecting to upstream, client: 172.17.0.1, server: , request: \"GET / HTTP/1.1\", upstream: \"http://127.0.0.1:15901/\", host: \"localhost:15901\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 172.17.0.1 - openai [28/Jun/2017:07:08:07 +0000] \"GET / HTTP/1.1\" 502 182 \"-\" \"AutobahnPython/17.6.2\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:07 [info] 64#64: *3 client 172.17.0.1 closed keepalive connection\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [vnc_recorder] [2017-06-28 07:08:08,016] Listening on 0.0.0.0:5899\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [reward_recorder] [2017-06-28 07:08:08,065] Listening on 0.0.0.0:15898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-28 03:08:08,359] [0:localhost:5901] Waiting on rewarder: failed to complete WebSocket handshake. Retry in 3s (slept 1s/7s): connection was closed uncleanly (WebSocket connection upgrade failed (502 - BadGateway))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:08 [error] 64#64: *5 connect() failed (111: Connection refused) while connecting to upstream, client: 172.17.0.1, server: , request: \"GET / HTTP/1.1\", upstream: \"http://127.0.0.1:15901/\", host: \"localhost:15901\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 172.17.0.1 - openai [28/Jun/2017:07:08:08 +0000] \"GET / HTTP/1.1\" 502 182 \"-\" \"AutobahnPython/17.6.2\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:08 [info] 64#64: *5 client 172.17.0.1 closed keepalive connection\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,460] [INFO:root] Starting play_controlplane.py with the following: command=['/app/universe-envs/controlplane/bin/controlplane.py', '--rewarder-port=15901'] args=Namespace(bot_demonstration=False, demonstration=False, env_id=None, idle_timeout=None, integrator_mode=False, no_env=False, no_rewarder=False, no_scorer=False, no_vexpect=False, remotes='vnc://127.0.0.1:5900', rewarder_fps=60, rewarder_port=15901, verbosity=0) env=environ({'HOSTNAME': 'de0780c14ac2', 'SUDO_GID': '0', 'SUDO_USER': 'root', 'LOGNAME': 'nobody', 'SHELL': '/usr/sbin/nologin', 'MAIL': '/var/mail/nobody', 'USER': 'nobody', 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin', 'HOME': '/nonexistent', 'DISPLAY': ':0', 'DBUS_SESSION_BUS_ADDRESS': '/dev/null', 'TERM': 'xterm', 'SUDO_UID': '0', 'USERNAME': 'nobody', 'SUDO_COMMAND': '/app/universe-envs/controlplane/bin/controlplane.py --rewarder-port=15901'})\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,461] [INFO:root] [EnvStatus] Changing env_state: None (env_id=None) -> None (env_id=None) (episode_id: 0->0, fps=60)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,465] [INFO:universe.rewarder.remote] Starting Rewarder on port=15901\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,481] [INFO:universe.extra.universe.wrappers.logger] Running VNC environments with Logger set to print_frequency=5. To change this, pass \"print_frequency=k\" or \"print_frequency=None\" to \"env.configure\".\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,492] [INFO:universe.remotes.hardcoded_addresses] No rewarder addresses were provided, so this env cannot connect to the remote's rewarder channel, and cannot send control messages (e.g. reset)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,493] [INFO:universe.envs.vnc_env] Using the golang VNC implementation\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,497] [INFO:universe.envs.vnc_env] Using VNCSession arguments: {'start_timeout': 7, 'subsample_level': 2, 'compress_level': 9, 'encoding': 'zrle', 'fine_quality_level': 50}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,501] [INFO:universe.envs.vnc_env] Printed stats will ignore clock skew. (This usually makes sense only when the environment and agent are on the same machine.)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,514] [INFO:universe.envs.vnc_env] [0] Connecting to environment: vnc://127.0.0.1:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://None/viewer/?password=openai\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,515] [INFO:universe.extra.universe.envs.vnc_env] [0] Connecting to environment details: vnc_address=127.0.0.1:5900 vnc_password=openai rewarder_address=None rewarder_password=openai\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m 2017/06/28 07:08:08 I0628 07:08:08.516768 60 gymvnc.go:417] [0:127.0.0.1:5900] opening connection to VNC server\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Wed Jun 28 07:08:08 2017\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  Connections: accepted: 127.0.0.1::40532\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  SConnection: Client needs protocol version 3.8\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,520] [INFO:root] [EnvStatus] Changing env_state: None (env_id=None) -> resetting (env_id=None) (episode_id: 0->1, fps=60)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,520] [INFO:root] [MainThread] Env state: env_id=None episode_id=1\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  SConnection: Client requests security type VncAuth(2)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,521] [INFO:root] [MainThread] Writing None to /tmp/demo/env_id.txt\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  VNCSConnST:  Server default pixel format depth 24 (32bpp) little-endian rgb888\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  VNCSConnST:  Client pixel format depth 24 (32bpp) little-endian bgr888\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m 2017/06/28 07:08:08 I0628 07:08:08.526504 60 gymvnc.go:550] [0:127.0.0.1:5900] connection established\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [Wed Jun 28 07:08:08 UTC 2017] [/usr/local/bin/sudoable-env-setup] Disabling outbound network traffic for none\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,605] [INFO:gym_flashgames.launcher] [MainThread] Launching new Chrome process (attempt 0/10)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,606] [INFO:root] Replacing selenium_wrapper_server since we currently do it at every episode boundary\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:08,819] [selenium_wrapper_server] Calling webdriver.Chrome()\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:10 [info] 64#64: *7 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:10 [info] 64#64: *8 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:10,566] [selenium_wrapper_server] Call to webdriver.Chrome() completed: 1.75s\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:10,568] [INFO:gym_flashgames.launcher] [MainThread] Navigating browser to url=http://localhost\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:11 [info] 64#64: *9 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:11,389] [INFO:universe.rewarder.remote] Client connecting: peer=tcp4:127.0.0.1:51032 observer=False\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:11,390] [INFO:universe.rewarder.remote] WebSocket connection established\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:11,499] [INFO:root] [EnvStatus] Changing env_state: resetting (env_id=None) -> running (env_id=None) (episode_id: 1->1, fps=60)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:11,507] [INFO:root] [MainThread] Writing None to /tmp/demo/env_id.txt\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m Manhole[1498633691.5207]: Patched <built-in function fork> and <built-in function fork>.\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m Manhole[1498633691.5221]: Manhole UDS path: /tmp/manhole-60\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m Manhole[1498633691.5223]: Waiting for new connection (in pid:60) ...\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:13,520] [INFO:universe.wrappers.logger] Stats for the past 5.00s: vnc_updates_ps=3.4 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=359024.1 vnc_pixels_ps[total]=528651.5 reward_lag=None rewarder_message_lag=None fps=24.20\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:16,518] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 301, \"std\": \"140.08us\", \"mean\": \"172.59us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"1.00ms\", \"mean\": \"15.32ms\"}, \"rewarder.compute_reward\": {\"calls\": 301, \"std\": \"384.98us\", \"mean\": \"441.90us\"}, \"rewarder.frame\": {\"calls\": 300, \"std\": \"832.96us\", \"mean\": \"17.43ms\"}} counters={\"reward.vnc.updates.n\": {\"calls\": 301, \"std\": 0.5034542915685413, \"mean\": 0.05647840531561465}} gauges={} (export_time=109.91us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:16,518] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.00s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.vnc.updates.pixels': 0, 'rewarder.profile': '<760 bytes>', 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:18,535] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=0.0 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=0.0 vnc_pixels_ps[total]=0.0 reward_lag=None rewarder_message_lag=None fps=60.04\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:21,519] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 300, \"std\": \"98.02us\", \"mean\": \"141.92us\"}, \"rewarder.sleep\": {\"calls\": 300, \"std\": \"627.89us\", \"mean\": \"15.43ms\"}, \"rewarder.compute_reward\": {\"calls\": 300, \"std\": \"221.55us\", \"mean\": \"364.86us\"}, \"rewarder.frame\": {\"calls\": 300, \"std\": \"575.81us\", \"mean\": \"17.43ms\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"std\": 0, \"mean\": 0.0}, \"reward.vnc.updates.n\": {\"calls\": 300, \"std\": 0.0, \"mean\": 0.0}} gauges={} (export_time=128.03us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:21,519] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.00s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.vnc.updates.pixels': 0, 'rewarder.profile': '<804 bytes>', 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:23,536] [INFO:universe.wrappers.logger] Stats for the past 5.00s: vnc_updates_ps=0.0 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=0.0 vnc_pixels_ps[total]=0.0 reward_lag=None rewarder_message_lag=None fps=60.00\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:25 [info] 64#64: *14 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:25 [info] 64#64: *15 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:25 [info] 64#64: *16 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:25 [info] 64#64: *17 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:25 [info] 64#64: *18 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:25 [info] 64#64: *19 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:25 [info] 64#64: *20 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:25 [info] 64#64: *21 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:25 [info] 64#64: *22 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:25 [info] 64#64: *23 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:25,785] [INFO:universe.rewarder.remote] CONNECTION STATUS: Marking connection as active: observer=False peer=tcp4:127.0.0.1:51032 total_conns=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-28 03:08:25,847] [0:localhost:5901] Sending reset for env_id=flashgames.DuskDrive-v0 fps=60 episode_id=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:25,851] [INFO:universe.rewarder.remote] Received reset message: {'body': {'fps': 60, 'env_id': 'flashgames.DuskDrive-v0', 'seed': None}, 'method': 'v0.env.reset', 'headers': {'message_id': 10, 'sent_at': 1498633705.849562, 'episode_id': '0'}}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:25,863] [INFO:root] [EnvStatus] Changing env_state: running (env_id=None) -> resetting (env_id=flashgames.DuskDrive-v0) (episode_id: 1->2, fps=60)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:25,865] [ERROR:root] Closing server (via subprocess.close()) and all chromes (via pkill chromedriver || :; pkill chrome || :)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:25,877] [INFO:root] [Rewarder] Blocking until env finishes resetting\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:08:25,890] init detected end of child process 108 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:08:25,906] init detected end of child process 123 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:25,906] [INFO:root] [EnvController] RESET CAUSE: changing out environments due to v0.env.reset (with episode_id=0): flashgames.DuskDrive-v0 -> flashgames.DuskDrive-v0 (new episode_id=2 fps=60)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:25,906] [INFO:root] [EnvController] Env state: env_id=flashgames.DuskDrive-v0 episode_id=2\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:08:25,908] init detected end of child process 330 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:25,910] [INFO:root] [EnvController] Writing flashgames.DuskDrive-v0 to /tmp/demo/env_id.txt\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:08:25,943] init detected end of child process 345 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [Wed Jun 28 07:08:25 UTC 2017] [/usr/local/bin/sudoable-env-setup] Allowing outbound network traffic to non-private IPs for git-lfs. (Going to fetch files via git lfs.)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:26 [info] 64#64: *13 client closed connection while waiting for request, client: 127.0.0.1, server: 0.0.0.0:80\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:26 [info] 64#64: *12 client 127.0.0.1 closed keepalive connection\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [unpack-lfs] [2017-06-28 07:08:26,107] Unpacking files for flashgames.DuskDrive-v0\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:08:26,122] init detected end of child process 119 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:08:26,125] init detected end of child process 120 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:08:26,126] init detected end of child process 111 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:08:26,127] init detected end of child process 122 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [unpack-lfs] [2017-06-28 07:08:26,215] Merged 5 files from /tmp/flashgames.DuskDrive-v0/public -> /app/universe-envs/flashgames/build/public/flashgames.DuskDrive-v0\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [unpack-lfs] [2017-06-28 07:08:26,217] Merged 20 files from /tmp/flashgames.DuskDrive-v0/private -> /app/universe-envs/flashgames/build/private/flashgames.DuskDrive-v0\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [unpack-lfs] [2017-06-28 07:08:26,219] Completed unpack for flashgames.DuskDrive-v0 in 0.113s\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [Wed Jun 28 07:08:26 UTC 2017] [/usr/local/bin/sudoable-env-setup] [debug] unpack-lfs completed with status code: 0. Created completion file: /usr/local/openai/git-lfs/flashgames.DuskDrive-v0\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [Wed Jun 28 07:08:26 UTC 2017] [/usr/local/bin/sudoable-env-setup] Disabling outbound network traffic for flashgames.DuskDrive-v0\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:26,371] [INFO:gym_flashgames.launcher] [EnvController] Launching new Chrome process (attempt 0/10)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:26,372] [INFO:root] Replacing selenium_wrapper_server since we currently do it at every episode boundary\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:26,504] [selenium_wrapper_server] Calling webdriver.Chrome()\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:28 [info] 64#64: *24 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:28 [info] 64#64: *25 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:28 [info] 64#64: *26 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:08:28 [info] 64#64: *27 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:28,574] [selenium_wrapper_server] Call to webdriver.Chrome() completed: 2.07s\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:28,581] [INFO:gym_flashgames.launcher] [EnvController] Navigating browser to url=http://localhost/flashgames.DuskDrive-v0\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:28,698] [INFO:root] [EnvController] Running command: /app/universe-envs/controlplane/bin/play_vexpect -e flashgames.DuskDrive-v0 -r vnc://127.0.0.1:5900 -d\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:29,926] [play_vexpect] No rewarder addresses were provided, so this env cannot connect to the remote's rewarder channel, and cannot send control messages (e.g. reset)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:29,926] [play_vexpect] Using the golang VNC implementation\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:29,927] [play_vexpect] Using VNCSession arguments: {'compress_level': 0, 'subsample_level': 2, 'encoding': 'zrle', 'fine_quality_level': 50, 'start_timeout': 7}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:29,928] [play_vexpect] Printed stats will ignore clock skew. (This usually makes sense only when the environment and agent are on the same machine.)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:29,932] [play_vexpect] [0] Connecting to environment: vnc://127.0.0.1:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://None/viewer/?password=openai\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:29,933] [play_vexpect] [0] Connecting to environment details: vnc_address=127.0.0.1:5900 vnc_password=openai rewarder_address=None rewarder_password=openai\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m 2017/06/28 07:08:29 I0628 07:08:29.934347 656 gymvnc.go:417] [0:127.0.0.1:5900] opening connection to VNC server\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Wed Jun 28 07:08:29 2017\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  Connections: accepted: 127.0.0.1::40772\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  SConnection: Client needs protocol version 3.8\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  SConnection: Client requests security type VncAuth(2)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  VNCSConnST:  Server default pixel format depth 24 (32bpp) little-endian rgb888\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m 2017/06/28 07:08:29 I0628 07:08:29.939293 656 gymvnc.go:550] [0:127.0.0.1:5900] connection established\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  VNCSConnST:  Client pixel format depth 24 (32bpp) little-endian bgr888\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:30,448] [play_vexpect] Waiting for any of [MaskState<initializing0>] to activate\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m 2017/06/28 07:08:34 I0628 07:08:34.801796 60 gymvnc.go:374] [0:127.0.0.1:5900] update queue max of 60 reached; pausing further updates\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:37,542] [play_vexpect] Applying transition: ClickTransition<initializing0->['initializing1'] x=429 y=539 buttonmask=1> for active state MaskState<initializing0>. (Summary: plausible_states=MaskState<initializing0> distance_m=0.0489 match_time_m=227us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:37,559] [play_vexpect] Waiting for any of [MaskState<initializing1>] to activate (or whether any of [MaskState<initializing0>] are still active)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:41,609] [play_vexpect] Advancing to the next hopeful state (2/2): MaskState<initializing0>\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:43,655] [play_vexpect] Advancing to the next hopeful state (1/2): MaskState<initializing1>\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:43,856] [play_vexpect] Applying transition: ClickTransition<initializing1->['initializing2'] x=571 y=512 buttonmask=1> for active state MaskState<initializing1>. (Summary: plausible_states=[MaskState<initializing1>, MaskState<initializing0>] distance_m=[0.0044, 0.7509666666666667] match_time_m=['268us', '289us'])\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:43,870] [play_vexpect] Waiting for any of [MaskState<initializing2>] to activate (or whether any of [MaskState<initializing1>] are still active)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:44,403] [play_vexpect] Applying transition: ClickTransition<initializing2->['ready0', 'ready1', 'ready2', 'ready3'] x=216 y=296 buttonmask=1> for active state MaskState<initializing2>. (Summary: plausible_states=[MaskState<initializing2>, MaskState<initializing1>] distance_m=[0.0359, 0.9961] match_time_m=['279us', '190us'])\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:44,420] [play_vexpect] Waiting for any of [ready0, ready1, ready2, ready3] to activate (or whether any of [MaskState<initializing2>] are still active)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:44,876] [play_vexpect] Applying transition: ClickTransition<ready1->[] x=0 y=0 buttonmask=0> for active state ready1. (Summary: plausible_states=[ready0, ready1, ready2, ready3, MaskState<initializing2>] distance_m=[0.31468353, 0.11631844, 0.28442386, 0.039567132, 0.9969] match_time_m=['200us', '104us', '320us', '121us', '168us'])\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:44,877] [play_vexpect] Reaching start state: ready1\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:44,878] [play_vexpect] vexpect macro complete in 14.904859s\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Wed Jun 28 07:08:44 2017\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  Connections: closed: 127.0.0.1::40772 (Clean disconnection)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager: Framebuffer updates: 214\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:   ZRLE:\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Solid: 33 rects, 1.22275 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:            2.74023 KiB (1:1743.19 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Wed Jun 28 07:08:45 2017\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Bitmap RLE: 82 rects, 1.28262 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:                 166.678 KiB (1:30.0653 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Indexed RLE: 127 rects, 938.549 kpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:                  269.209 KiB (1:13.624 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Full Colour: 215 rects, 8.35779 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:                  23.9229 MiB (1:1.33282 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:   Total: 457 rects, 11.8017 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:          24.3513 MiB (1:1.84899 ratio)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-28 03:08:45,431] [0:localhost:5901] Initial reset complete: episode_id=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:45,418] [INFO:root] [EnvStatus] Changing env_state: resetting (env_id=flashgames.DuskDrive-v0) -> running (env_id=flashgames.DuskDrive-v0) (episode_id: 2->2, fps=60)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:45,420] [INFO:universe.rewarder.remote] Sending rewarder message: {'body': {}, 'method': 'v0.reply.env.reset', 'headers': {'message_id': 16, 'parent_message_id': 10, 'parent_runtime': 19.569300174713135, 'sent_at': 1498633725.4206817, 'episode_id': '2'}}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:45,425] [INFO:root] [Rewarder] Unblocking since env reset finished\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:45,426] [INFO:root] [Rewarder] Changing reward_parsers: None -> flashgames.DuskDrive-v0\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:45,430] [INFO:root] [Rewarder] Writing flashgames.DuskDrive-v0 to /tmp/demo/env_id.txt\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:48,300] [INFO:gym_controlplane.registration] Loaded scorer: <gym_controlplane.reward.score.OCRScorerV0 object at 0x7f79c5791400>\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:48,937] [INFO:gym_controlplane.registration] Created reward parser for flashgames.DuskDrive-v0: Reward<scorer=<gym_controlplane.reward.score.OCRScorerV0 object at 0x7f79c5791400> vexpect=VExpect<{'initializing2': <gym_controlplane.integration.transition.ClickTransition object at 0x7f7992169358>, 'ready3': <gym_controlplane.integration.transition.ClickTransition object at 0x7f799216acf8>, 'ready0': <gym_controlplane.integration.transition.ClickTransition object at 0x7f7992169ba8>, 'initializing1': <gym_controlplane.integration.transition.ClickTransition object at 0x7f7992165748>, 'initializing0': <gym_controlplane.integration.transition.ClickTransition object at 0x7f7992161da0>, 'ready2': <gym_controlplane.integration.transition.ClickTransition object at 0x7f799216a748>, 'ready1': <gym_controlplane.integration.transition.ClickTransition object at 0x7f799216a198>}>>\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:48,939] [INFO:root] Using metadata_encoding={'x': 914, 'width': 100, 'y': 658, 'type': 'qrcode', 'height': 100} probe_key=96 subscription=[(548, 100, 442, 100), (82, 92, 128, 20), (914, 100, 658, 100)]\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:48,939] [INFO:universe.rewarder.remote] [Rewarder] Over past 27.42s, sent 0 reward messages to agent: reward=0 reward_min=(empty) reward_max=(empty) done=False info={}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:48,940] [INFO:universe.rewarder.remote] [Rewarder] Ending previous episode: episode_reward=0 episode_count=2 episode_duration=37.42\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:48,943] [INFO:universe.wrappers.logger] Stats for the past 25.41s: vnc_updates_ps=0.0 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=0.0 vnc_pixels_ps[total]=0.0 reward_lag=None rewarder_message_lag=None fps=5.39\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m 2017/06/28 07:08:48 I0628 07:08:48.946515 60 gymvnc.go:278] [0:127.0.0.1:5900] resuming updates\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:48,983] [INFO:universe.pyprofile] [pyprofile] period=27.46s timers={\"rewarder.sleep.missed\": {\"calls\": 2, \"std\": \"40.68ms\", \"mean\": \"29.82ms\"}, \"rewarder_protocol.latency.rtt.skew_unadjusted\": {\"calls\": 11, \"std\": \"3.33ms\", \"mean\": \"4.31ms\"}, \"rewarder.compute_reward\": {\"calls\": 258, \"std\": \"2.67ms\", \"mean\": \"624.58us\"}, \"rewarder.frame\": {\"calls\": 258, \"std\": \"5.90ms\", \"mean\": \"18.09ms\"}, \"reward.parsing.score\": {\"calls\": 1, \"std\": \"0.00us\", \"mean\": \"38.50ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 258, \"std\": \"369.48us\", \"mean\": \"195.91us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 1, \"std\": \"0.00us\", \"mean\": \"4.53us\"}, \"rewarder.sleep\": {\"calls\": 256, \"std\": \"1.30ms\", \"mean\": \"15.20ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 1, \"std\": \"0.00us\", \"mean\": \"41.72us\"}, \"reward.parsing.gameover\": {\"calls\": 1, \"std\": \"0.00us\", \"mean\": \"200.03us\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 1, \"std\": \"0.00us\", \"mean\": \"100.61us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 1, \"std\": \"0.00us\", \"mean\": \"38.25ms\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"std\": 0, \"mean\": 0.0}, \"control.env_id_change\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}, \"control.env_id_change.flashgames.DuskDrive-v0\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}, \"reward.vnc.updates.n\": {\"calls\": 258, \"std\": 3.797694118824611, \"mean\": 0.2364341085271318}, \"rewarder_protocol.messages\": {\"calls\": 11, \"std\": 0.0, \"mean\": 1.0}, \"rewarder_protocol.messages.v0.control.ping\": {\"calls\": 10, \"std\": 0.0, \"mean\": 1.0}, \"rewarder_protocol.messages.v0.env.reset\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}} gauges={} (export_time=116.59us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:48,984] [INFO:root] [Rewarder] Rewarder fell behind by 3.541060209274292s from target; losing 212 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:49,012] [INFO:gym_controlplane.reward.reward] First score parsed: score=11691\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:49,997] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 15 reward messages to agent: reward=76.0 reward_min=-8.0 reward_max=32 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<2363 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:51,018] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 10 reward messages to agent: reward=22.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:52,073] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.05s, sent 10 reward messages to agent: reward=19.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 35041, 'rewarder.vnc.updates.pixels': 21020, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:53,094] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 8 reward messages to agent: reward=20.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:53,947] [INFO:universe.wrappers.logger] Stats for the past 5.00s: vnc_updates_ps=24.4 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=956241.7 vnc_pixels_ps[total]=912301.0 reward_lag=None rewarder_message_lag=None fps=57.58\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:54,021] [INFO:universe.pyprofile] [pyprofile] period=5.03s timers={\"rewarder.sleep.missed\": {\"calls\": 22, \"std\": \"753.35ms\", \"mean\": \"168.40ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 61, \"std\": \"133.48us\", \"mean\": \"119.17us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 61, \"std\": \"78.53us\", \"mean\": \"69.95us\"}, \"rewarder.sleep\": {\"calls\": 269, \"std\": \"4.85ms\", \"mean\": \"13.66ms\"}, \"rewarder.compute_reward\": {\"calls\": 291, \"std\": \"7.44ms\", \"mean\": \"3.90ms\"}, \"rewarder.frame\": {\"calls\": 291, \"std\": \"207.62ms\", \"mean\": \"29.99ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 61, \"std\": \"133.45us\", \"mean\": \"135.21us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 291, \"std\": \"3.11ms\", \"mean\": \"368.30us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 58, \"std\": \"5.36ms\", \"mean\": \"14.66ms\"}, \"reward.parsing.score\": {\"calls\": 61, \"std\": \"6.14ms\", \"mean\": \"14.43ms\"}, \"reward.parsing.gameover\": {\"calls\": 61, \"std\": \"214.08us\", \"mean\": \"300.42us\"}} counters={\"agent_conn.reward\": {\"calls\": 47, \"std\": 5.4302404701235165, \"mean\": 3.234042553191489}, \"reward.vnc.updates.n\": {\"calls\": 291, \"std\": 0.4077400220527781, \"mean\": 0.20962199312714785}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 3, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 60, \"std\": 31.112058265109113, \"value\": 11836.0, \"mean\": 11788.350000000002}} (export_time=9.40ms)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:54,421] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.33s, sent 6 reward messages to agent: reward=19.0 reward_min=0 reward_max=7.0 done=False info={'rewarder.vnc.updates.bytes': 5540, 'rewarder.vnc.updates.pixels': 1840, 'rewarder.profile': '<2059 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:55,454] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 8 reward messages to agent: reward=47.0 reward_min=1.0 reward_max=11.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:56,473] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 18 reward messages to agent: reward=709.0 reward_min=12.0 reward_max=78.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:57,521] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.05s, sent 14 reward messages to agent: reward=943.0 reward_min=4.0 reward_max=176.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:58,529] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.01s, sent 9 reward messages to agent: reward=15.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 36908, 'rewarder.vnc.updates.pixels': 21840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:58,952] [INFO:universe.wrappers.logger] Stats for the past 5.00s: vnc_updates_ps=12.2 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=425171.3 vnc_pixels_ps[total]=190265.9 reward_lag=None rewarder_message_lag=None fps=58.37\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:59,018] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"rewarder.sleep.missed\": {\"calls\": 12, \"std\": \"26.09ms\", \"mean\": \"12.60ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 60, \"std\": \"88.80us\", \"mean\": \"112.77us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 62, \"std\": \"99.66us\", \"mean\": \"75.08us\"}, \"rewarder.sleep\": {\"calls\": 281, \"std\": \"4.91ms\", \"mean\": \"13.52ms\"}, \"rewarder.compute_reward\": {\"calls\": 293, \"std\": \"6.76ms\", \"mean\": \"3.39ms\"}, \"rewarder.frame\": {\"calls\": 293, \"std\": \"5.71ms\", \"mean\": \"17.54ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 62, \"std\": \"179.62us\", \"mean\": \"133.36us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 293, \"std\": \"916.95us\", \"mean\": \"221.56us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 60, \"std\": \"2.50ms\", \"mean\": \"12.05ms\"}, \"reward.parsing.score\": {\"calls\": 62, \"std\": \"3.30ms\", \"mean\": \"12.19ms\"}, \"reward.parsing.gameover\": {\"calls\": 62, \"std\": \"238.39us\", \"mean\": \"304.86us\"}} counters={\"agent_conn.reward\": {\"calls\": 56, \"std\": 39.991261220746395, \"mean\": 31.410714285714278}, \"reward.vnc.updates.n\": {\"calls\": 293, \"std\": 0.4091443881982594, \"mean\": 0.21160409556313986}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 2, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 2, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 62, \"std\": 743.4485604469725, \"value\": 13593.0, \"mean\": 12762.483870967739}} (export_time=143.05us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:08:59,690] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.16s, sent 11 reward messages to agent: reward=51.0 reward_min=0 reward_max=13.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<2192 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:00,753] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 6 reward messages to agent: reward=9.0 reward_min=1.0 reward_max=2.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:01,770] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 7 reward messages to agent: reward=30.0 reward_min=1.0 reward_max=10.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:02,800] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 8 reward messages to agent: reward=20.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:03,881] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.08s, sent 15 reward messages to agent: reward=194.0 reward_min=2.0 reward_max=40.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:03,890] [INFO:root] [Rewarder] Rewarder fell behind by 0.11391830444335938s from target; losing 6 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:03,982] [INFO:universe.wrappers.logger] Stats for the past 5.03s: vnc_updates_ps=11.1 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=400694.3 vnc_pixels_ps[total]=183823.8 reward_lag=None rewarder_message_lag=None fps=54.31\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:04,105] [INFO:universe.pyprofile] [pyprofile] period=5.03s timers={\"rewarder.sleep.missed\": {\"calls\": 50, \"std\": \"16.27ms\", \"mean\": \"9.24ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 55, \"std\": \"53.05us\", \"mean\": \"101.82us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 55, \"std\": \"28.04us\", \"mean\": \"60.99us\"}, \"rewarder.sleep\": {\"calls\": 221, \"std\": \"2.32ms\", \"mean\": \"15.53ms\"}, \"rewarder.compute_reward\": {\"calls\": 271, \"std\": \"11.54ms\", \"mean\": \"5.29ms\"}, \"rewarder.frame\": {\"calls\": 271, \"std\": \"8.12ms\", \"mean\": \"18.75ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 55, \"std\": \"36.17us\", \"mean\": \"88.45us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 271, \"std\": \"1.56ms\", \"mean\": \"264.82us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 55, \"std\": \"12.70ms\", \"mean\": \"22.49ms\"}, \"reward.parsing.score\": {\"calls\": 55, \"std\": \"12.71ms\", \"mean\": \"22.91ms\"}, \"reward.parsing.gameover\": {\"calls\": 55, \"std\": \"105.23us\", \"mean\": \"269.40us\"}} counters={\"agent_conn.reward\": {\"calls\": 42, \"std\": 8.616658635427596, \"mean\": 6.261904761904763}, \"reward.vnc.updates.n\": {\"calls\": 271, \"std\": 0.4029412160802568, \"mean\": 0.20295202952029517}} gauges={\"reward_parser.score.last_score\": {\"calls\": 55, \"std\": 48.26257446796738, \"value\": 13825.0, \"mean\": 13658.272727272728}} (export_time=9.25ms)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:04,123] [INFO:root] [Rewarder] Rewarder fell behind by 0.10419869422912598s from target; losing 6 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:05,156] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.27s, sent 3 reward messages to agent: reward=72.0 reward_min=0 reward_max=39.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.profile': '<1928 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:06,219] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 12 reward messages to agent: reward=217.0 reward_min=1.0 reward_max=98.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:07,225] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 15 reward messages to agent: reward=259.0 reward_min=3.0 reward_max=53.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:08,345] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.12s, sent 13 reward messages to agent: reward=689.0 reward_min=3.0 reward_max=112.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:09,002] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=10.2 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=358237.3 vnc_pixels_ps[total]=164049.4 reward_lag=None rewarder_message_lag=None fps=57.22\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:09,054] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"rewarder.sleep.missed\": {\"calls\": 13, \"std\": \"31.74ms\", \"mean\": \"19.23ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 51, \"std\": \"62.38us\", \"mean\": \"106.09us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 52, \"std\": \"85.81us\", \"mean\": \"76.26us\"}, \"rewarder.sleep\": {\"calls\": 275, \"std\": \"4.76ms\", \"mean\": \"13.61ms\"}, \"rewarder.compute_reward\": {\"calls\": 288, \"std\": \"5.66ms\", \"mean\": \"3.01ms\"}, \"rewarder.frame\": {\"calls\": 288, \"std\": \"8.06ms\", \"mean\": \"18.22ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 52, \"std\": \"90.39us\", \"mean\": \"108.15us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 288, \"std\": \"128.62us\", \"mean\": \"159.66us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 51, \"std\": \"3.77ms\", \"mean\": \"13.31ms\"}, \"reward.parsing.score\": {\"calls\": 52, \"std\": \"4.18ms\", \"mean\": \"13.54ms\"}, \"reward.parsing.gameover\": {\"calls\": 52, \"std\": \"183.64us\", \"mean\": \"300.22us\"}} counters={\"agent_conn.reward\": {\"calls\": 49, \"std\": 31.817880995132402, \"mean\": 25.551020408163264}, \"reward.vnc.updates.n\": {\"calls\": 288, \"std\": 0.38531905016750734, \"mean\": 0.18055555555555558}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 52, \"std\": 428.8384202104186, \"value\": 15116.0, \"mean\": 14471.038461538461}} (export_time=281.81us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:09,459] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.11s, sent 12 reward messages to agent: reward=24.0 reward_min=0 reward_max=7.0 done=False info={'rewarder.vnc.updates.bytes': 33721, 'rewarder.vnc.updates.pixels': 11220, 'rewarder.profile': '<2157 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:10,480] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 11 reward messages to agent: reward=25.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:11,530] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.05s, sent 11 reward messages to agent: reward=19.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 35041, 'rewarder.vnc.updates.pixels': 21020, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:12,566] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 6 reward messages to agent: reward=17.0 reward_min=1.0 reward_max=8.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:13,716] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.15s, sent 8 reward messages to agent: reward=267.0 reward_min=11.0 reward_max=75.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:14,298] [INFO:universe.wrappers.logger] Stats for the past 5.25s: vnc_updates_ps=11.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=393737.3 vnc_pixels_ps[total]=175526.0 reward_lag=None rewarder_message_lag=None fps=56.59\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:14,309] [INFO:universe.pyprofile] [pyprofile] period=5.25s timers={\"rewarder.sleep.missed\": {\"calls\": 19, \"std\": \"2.99ms\", \"mean\": \"3.39ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 59, \"std\": \"84.48us\", \"mean\": \"102.52us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 60, \"std\": \"42.01us\", \"mean\": \"62.34us\"}, \"rewarder.sleep\": {\"calls\": 275, \"std\": \"4.48ms\", \"mean\": \"14.05ms\"}, \"rewarder.compute_reward\": {\"calls\": 294, \"std\": \"6.73ms\", \"mean\": \"3.45ms\"}, \"rewarder.frame\": {\"calls\": 294, \"std\": \"13.63ms\", \"mean\": \"17.99ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 60, \"std\": \"129.21us\", \"mean\": \"123.02us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 294, \"std\": \"220.89us\", \"mean\": \"154.21us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 57, \"std\": \"3.86ms\", \"mean\": \"13.87ms\"}, \"reward.parsing.score\": {\"calls\": 60, \"std\": \"4.81ms\", \"mean\": \"13.69ms\"}, \"reward.parsing.gameover\": {\"calls\": 60, \"std\": \"173.38us\", \"mean\": \"272.31us\"}} counters={\"agent_conn.reward\": {\"calls\": 43, \"std\": 20.66328161467791, \"mean\": 10.069767441860465}, \"reward.vnc.updates.n\": {\"calls\": 294, \"std\": 0.40371610865190727, \"mean\": 0.20408163265306137}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 3, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 60, \"std\": 60.09007128192439, \"value\": 15454.0, \"mean\": 15167.616666666665}} (export_time=3.19ms)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:14,325] [INFO:root] [Rewarder] Rewarder fell behind by 0.2905306816101074s from target; losing 17 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:14,765] [INFO:root] [Rewarder] Rewarder fell behind by 0.1030726432800293s from target; losing 6 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:15,063] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.35s, sent 3 reward messages to agent: reward=238.0 reward_min=0 reward_max=142.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.profile': '<2169 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:16,082] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 1 reward messages to agent: reward=442.0 reward_min=442.0 reward_max=442.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:17,085] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 7 reward messages to agent: reward=284.0 reward_min=1.0 reward_max=161.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:18,126] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 6 reward messages to agent: reward=8.0 reward_min=1.0 reward_max=2.0 done=False info={'rewarder.vnc.updates.bytes': 33721, 'rewarder.vnc.updates.pixels': 11220, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:19,203] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.08s, sent 12 reward messages to agent: reward=96.0 reward_min=1.0 reward_max=26.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:19,309] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"rewarder.sleep.missed\": {\"calls\": 15, \"std\": \"76.35ms\", \"mean\": \"33.45ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 40, \"std\": \"116.28us\", \"mean\": \"103.52us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 40, \"std\": \"14.31us\", \"mean\": \"51.34us\"}, \"rewarder.sleep\": {\"calls\": 270, \"std\": \"3.97ms\", \"mean\": \"14.37ms\"}, \"rewarder.compute_reward\": {\"calls\": 285, \"std\": \"5.98ms\", \"mean\": \"2.78ms\"}, \"rewarder.frame\": {\"calls\": 285, \"std\": \"20.84ms\", \"mean\": \"19.31ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 40, \"std\": \"55.34us\", \"mean\": \"112.87us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 285, \"std\": \"296.28us\", \"mean\": \"182.23us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 39, \"std\": \"3.90ms\", \"mean\": \"13.80ms\"}, \"reward.parsing.score\": {\"calls\": 40, \"std\": \"4.47ms\", \"mean\": \"13.91ms\"}, \"reward.parsing.gameover\": {\"calls\": 40, \"std\": \"193.52us\", \"mean\": \"273.69us\"}} counters={\"agent_conn.reward\": {\"calls\": 28, \"std\": 89.02636194798212, \"mean\": 34.714285714285715}, \"reward.vnc.updates.n\": {\"calls\": 285, \"std\": 0.3479616950763762, \"mean\": 0.14035087719298248}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 40, \"std\": 186.0219159269376, \"value\": 16522.0, \"mean\": 16378.274999999998}} (export_time=454.66us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:19,311] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=8.0 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=276549.0 vnc_pixels_ps[total]=129144.6 reward_lag=None rewarder_message_lag=None fps=57.08\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:20,262] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 10 reward messages to agent: reward=1141.0 reward_min=28.0 reward_max=190.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.profile': '<2041 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:21,277] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 11 reward messages to agent: reward=1765.0 reward_min=19.0 reward_max=414.0 done=False info={'rewarder.vnc.updates.bytes': 35067, 'rewarder.vnc.updates.pixels': 21220, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:22,365] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.09s, sent 10 reward messages to agent: reward=35.0 reward_min=1.0 reward_max=13.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:23,496] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.13s, sent 8 reward messages to agent: reward=50.0 reward_min=1.0 reward_max=16.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:24,496] [INFO:universe.pyprofile] [pyprofile] period=5.19s timers={\"rewarder.sleep.missed\": {\"calls\": 4, \"std\": \"2.17ms\", \"mean\": \"2.13ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 55, \"std\": \"5.09ms\", \"mean\": \"780.04us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 55, \"std\": \"4.15ms\", \"mean\": \"615.76us\"}, \"rewarder.sleep\": {\"calls\": 293, \"std\": \"4.74ms\", \"mean\": \"13.86ms\"}, \"rewarder.compute_reward\": {\"calls\": 297, \"std\": \"14.89ms\", \"mean\": \"3.39ms\"}, \"rewarder.frame\": {\"calls\": 297, \"std\": \"367.28us\", \"mean\": \"16.96ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 55, \"std\": \"43.11us\", \"mean\": \"84.47us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 297, \"std\": \"147.75us\", \"mean\": \"127.61us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 54, \"std\": \"6.54ms\", \"mean\": \"12.68ms\"}, \"reward.parsing.score\": {\"calls\": 55, \"std\": \"21.39ms\", \"mean\": \"14.88ms\"}, \"reward.parsing.gameover\": {\"calls\": 55, \"std\": \"10.05ms\", \"mean\": \"1.60ms\"}} counters={\"agent_conn.reward\": {\"calls\": 44, \"std\": 100.23157910717124, \"mean\": 68.15909090909093}, \"reward.vnc.updates.n\": {\"calls\": 297, \"std\": 0.4003422414308915, \"mean\": 0.18855218855218853}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 55, \"std\": 988.9078089212638, \"value\": 19521.0, \"mean\": 18925.89090909091}} (export_time=461.82us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:24,502] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.01s, sent 6 reward messages to agent: reward=9.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<2039 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:24,506] [INFO:root] [Rewarder] Rewarder fell behind by 0.23861145973205566s from target; losing 14 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:24,541] [INFO:universe.wrappers.logger] Stats for the past 5.21s: vnc_updates_ps=10.7 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=377417.0 vnc_pixels_ps[total]=172327.3 reward_lag=None rewarder_message_lag=None fps=56.95\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:24,812] [INFO:root] [Rewarder] Rewarder fell behind by 0.1363391876220703s from target; losing 8 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:25,618] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.12s, sent 2 reward messages to agent: reward=3.0 reward_min=1.0 reward_max=2.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:26,638] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 5 reward messages to agent: reward=33.0 reward_min=1.0 reward_max=24.0 done=False info={'rewarder.vnc.updates.bytes': 36570, 'rewarder.vnc.updates.pixels': 19240, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:27,711] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 10 reward messages to agent: reward=23.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 33721, 'rewarder.vnc.updates.pixels': 11220, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:28,769] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 8 reward messages to agent: reward=19.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 36908, 'rewarder.vnc.updates.pixels': 21840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:29,523] [INFO:universe.pyprofile] [pyprofile] period=5.03s timers={\"rewarder.sleep.missed\": {\"calls\": 19, \"std\": \"60.36ms\", \"mean\": \"24.95ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 41, \"std\": \"63.08us\", \"mean\": \"98.43us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 43, \"std\": \"38.46us\", \"mean\": \"65.32us\"}, \"rewarder.sleep\": {\"calls\": 266, \"std\": \"4.09ms\", \"mean\": \"14.38ms\"}, \"rewarder.compute_reward\": {\"calls\": 285, \"std\": \"10.12ms\", \"mean\": \"3.37ms\"}, \"rewarder.frame\": {\"calls\": 285, \"std\": \"17.95ms\", \"mean\": \"18.96ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 43, \"std\": \"105.82us\", \"mean\": \"121.16us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 285, \"std\": \"152.00us\", \"mean\": \"146.89us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 40, \"std\": \"4.08ms\", \"mean\": \"14.48ms\"}, \"reward.parsing.score\": {\"calls\": 43, \"std\": \"5.43ms\", \"mean\": \"13.97ms\"}, \"reward.parsing.gameover\": {\"calls\": 43, \"std\": \"167.62us\", \"mean\": \"274.39us\"}} counters={\"agent_conn.reward\": {\"calls\": 31, \"std\": 4.162815424453084, \"mean\": 2.9354838709677415}, \"reward.vnc.updates.n\": {\"calls\": 285, \"std\": 0.3585587381003972, \"mean\": 0.15087719298245617}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 2, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 3, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 43, \"std\": 26.417279321180963, \"value\": 19612.0, \"mean\": 19572.279069767435}} (export_time=113.01us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:29,547] [INFO:universe.wrappers.logger] Stats for the past 5.00s: vnc_updates_ps=8.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=283800.0 vnc_pixels_ps[total]=135542.7 reward_lag=None rewarder_message_lag=None fps=57.15\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:29,841] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 9 reward messages to agent: reward=20.0 reward_min=0.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.profile': '<2193 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:30,864] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 13 reward messages to agent: reward=27.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 36908, 'rewarder.vnc.updates.pixels': 21840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:31,918] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.05s, sent 10 reward messages to agent: reward=14.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 33721, 'rewarder.vnc.updates.pixels': 11220, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:32,952] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 9 reward messages to agent: reward=20.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 33721, 'rewarder.vnc.updates.pixels': 11220, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:33,984] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 8 reward messages to agent: reward=18.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:34,560] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=12.0 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=427091.5 vnc_pixels_ps[total]=192840.4 reward_lag=None rewarder_message_lag=None fps=59.47\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:34,561] [INFO:universe.pyprofile] [pyprofile] period=5.04s timers={\"rewarder.sleep.missed\": {\"calls\": 6, \"std\": \"2.28ms\", \"mean\": \"1.43ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 60, \"std\": \"47.59us\", \"mean\": \"110.95us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 60, \"std\": \"116.95us\", \"mean\": \"86.64us\"}, \"rewarder.sleep\": {\"calls\": 294, \"std\": \"5.04ms\", \"mean\": \"13.46ms\"}, \"rewarder.compute_reward\": {\"calls\": 300, \"std\": \"5.33ms\", \"mean\": \"3.02ms\"}, \"rewarder.frame\": {\"calls\": 300, \"std\": \"2.20ms\", \"mean\": \"17.13ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 60, \"std\": \"58.13us\", \"mean\": \"102.06us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 300, \"std\": \"132.64us\", \"mean\": \"140.79us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 60, \"std\": \"2.11ms\", \"mean\": \"12.27ms\"}, \"reward.parsing.score\": {\"calls\": 60, \"std\": \"2.08ms\", \"mean\": \"12.73ms\"}, \"reward.parsing.gameover\": {\"calls\": 60, \"std\": \"182.98us\", \"mean\": \"321.21us\"}} counters={\"agent_conn.reward\": {\"calls\": 49, \"std\": 1.4433756729740645, \"mean\": 2.142857142857143}, \"reward.vnc.updates.n\": {\"calls\": 300, \"std\": 0.40066833797650664, \"mean\": 0.2000000000000001}} gauges={\"reward_parser.score.last_score\": {\"calls\": 60, \"std\": 26.971024418663035, \"value\": 19716.0, \"mean\": 19659.766666666666}} (export_time=280.38us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:34,933] [INFO:root] [Rewarder] Rewarder fell behind by 0.26987791061401367s from target; losing 16 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:35,171] [INFO:root] [Rewarder] Rewarder fell behind by 0.10417938232421875s from target; losing 6 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:35,975] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.99s, sent 7 reward messages to agent: reward=19.0 reward_min=0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 36908, 'rewarder.vnc.updates.pixels': 21840, 'rewarder.profile': '<1937 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:37,382] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.41s, sent 4 reward messages to agent: reward=21.0 reward_min=1.0 reward_max=17.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:38,876] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.49s, sent 1 reward messages to agent: reward=15.0 reward_min=15.0 reward_max=15.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:39,564] [INFO:universe.wrappers.logger] Stats for the past 5.00s: vnc_updates_ps=2.0 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=66374.3 vnc_pixels_ps[total]=39088.1 reward_lag=None rewarder_message_lag=None fps=54.37\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:39,567] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"rewarder.sleep.missed\": {\"calls\": 12, \"std\": \"77.34ms\", \"mean\": \"42.36ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 9, \"std\": \"378.83us\", \"mean\": \"220.48us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 10, \"std\": \"125.06us\", \"mean\": \"114.73us\"}, \"rewarder.sleep\": {\"calls\": 260, \"std\": \"1.87ms\", \"mean\": \"15.29ms\"}, \"rewarder.compute_reward\": {\"calls\": 272, \"std\": \"14.37ms\", \"mean\": \"2.27ms\"}, \"rewarder.frame\": {\"calls\": 272, \"std\": \"18.33ms\", \"mean\": \"19.41ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 10, \"std\": \"56.18us\", \"mean\": \"105.52us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 272, \"std\": \"711.90us\", \"mean\": \"264.17us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 9, \"std\": \"4.05ms\", \"mean\": \"20.02ms\"}, \"reward.parsing.score\": {\"calls\": 10, \"std\": \"7.56ms\", \"mean\": \"18.62ms\"}, \"reward.parsing.gameover\": {\"calls\": 10, \"std\": \"1.10ms\", \"mean\": \"649.88us\"}} counters={\"agent_conn.reward\": {\"calls\": 8, \"std\": 6.823017765517794, \"mean\": 5.375}, \"reward.vnc.updates.n\": {\"calls\": 272, \"std\": 0.18853046912984406, \"mean\": 0.03676470588235294}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 10, \"std\": 13.233375314792886, \"value\": 19754.0, \"mean\": 19729.700000000004}} (export_time=629.19us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:40,088] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.21s, sent 4 reward messages to agent: reward=300.0 reward_min=0 reward_max=234.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.profile': '<2136 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:41,143] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.05s, sent 8 reward messages to agent: reward=6337.0 reward_min=475.0 reward_max=1223.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:42,208] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 9 reward messages to agent: reward=5834.0 reward_min=136.0 reward_max=1431.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:43,255] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.05s, sent 7 reward messages to agent: reward=234.0 reward_min=2.0 reward_max=111.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:44,295] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 8 reward messages to agent: reward=70.0 reward_min=1.0 reward_max=20.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:44,574] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=7.4 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=266357.8 vnc_pixels_ps[total]=127914.0 reward_lag=None rewarder_message_lag=None fps=58.89\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:44,575] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"rewarder.sleep.missed\": {\"calls\": 17, \"std\": \"5.36ms\", \"mean\": \"5.50ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 37, \"std\": \"148.63us\", \"mean\": \"117.72us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 37, \"std\": \"48.16us\", \"mean\": \"63.14us\"}, \"rewarder.sleep\": {\"calls\": 278, \"std\": \"3.31ms\", \"mean\": \"15.03ms\"}, \"rewarder.compute_reward\": {\"calls\": 295, \"std\": \"5.63ms\", \"mean\": \"2.34ms\"}, \"rewarder.frame\": {\"calls\": 295, \"std\": \"1.79ms\", \"mean\": \"17.30ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 37, \"std\": \"42.50us\", \"mean\": \"88.87us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 295, \"std\": \"191.45us\", \"mean\": \"127.81us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 37, \"std\": \"4.90ms\", \"mean\": \"15.07ms\"}, \"reward.parsing.score\": {\"calls\": 37, \"std\": \"5.00ms\", \"mean\": \"15.49ms\"}, \"reward.parsing.gameover\": {\"calls\": 37, \"std\": \"383.86us\", \"mean\": \"325.74us\"}} counters={\"agent_conn.reward\": {\"calls\": 36, \"std\": 427.3786284589954, \"mean\": 354.7222222222221}, \"reward.vnc.updates.n\": {\"calls\": 295, \"std\": 0.33176154268349994, \"mean\": 0.1254237288135594}} gauges={\"reward_parser.score.last_score\": {\"calls\": 37, \"std\": 4537.254812243988, \"value\": 32529.0, \"mean\": 29085.864864864863}} (export_time=562.19us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:45,202] [INFO:root] [Rewarder] Rewarder fell behind by 0.12547874450683594s from target; losing 7 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:45,758] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.46s, sent 4 reward messages to agent: reward=6.0 reward_min=0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<1936 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:46,795] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 3 reward messages to agent: reward=67.0 reward_min=5.0 reward_max=53.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:47,862] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 7 reward messages to agent: reward=93.0 reward_min=2.0 reward_max=28.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:48,881] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 9 reward messages to agent: reward=26.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:49,594] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"rewarder.sleep.missed\": {\"calls\": 21, \"std\": \"28.07ms\", \"mean\": \"13.01ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 33, \"std\": \"102.27us\", \"mean\": \"128.65us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 33, \"std\": \"29.65us\", \"mean\": \"61.03us\"}, \"rewarder.sleep\": {\"calls\": 262, \"std\": \"3.32ms\", \"mean\": \"14.75ms\"}, \"rewarder.compute_reward\": {\"calls\": 283, \"std\": \"7.00ms\", \"mean\": \"2.86ms\"}, \"rewarder.frame\": {\"calls\": 283, \"std\": \"8.29ms\", \"mean\": \"18.17ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 33, \"std\": \"162.25us\", \"mean\": \"121.17us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 283, \"std\": \"309.49us\", \"mean\": \"176.49us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 33, \"std\": \"4.75ms\", \"mean\": \"15.02ms\"}, \"reward.parsing.score\": {\"calls\": 33, \"std\": \"4.76ms\", \"mean\": \"15.51ms\"}, \"reward.parsing.gameover\": {\"calls\": 33, \"std\": \"153.59us\", \"mean\": \"307.89us\"}} counters={\"agent_conn.reward\": {\"calls\": 30, \"std\": 10.667869903399744, \"mean\": 8.3}, \"reward.vnc.updates.n\": {\"calls\": 283, \"std\": 0.32152089518201415, \"mean\": 0.1166077738515901}} gauges={\"reward_parser.score.last_score\": {\"calls\": 33, \"std\": 77.78515477281171, \"value\": 32779.0, \"mean\": 32680.696969696968}} (export_time=767.95us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:49,600] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=6.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=234802.6 vnc_pixels_ps[total]=109842.4 reward_lag=None rewarder_message_lag=None fps=56.54\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:50,021] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.14s, sent 10 reward messages to agent: reward=61.0 reward_min=1.0 reward_max=12.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.profile': '<1932 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:51,143] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.12s, sent 9 reward messages to agent: reward=75.0 reward_min=1.0 reward_max=23.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:52,172] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 6 reward messages to agent: reward=43.0 reward_min=2.0 reward_max=12.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:53,243] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 8 reward messages to agent: reward=52.0 reward_min=1.0 reward_max=12.0 done=False info={'rewarder.vnc.updates.bytes': 36908, 'rewarder.vnc.updates.pixels': 21840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:54,287] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 14 reward messages to agent: reward=149.0 reward_min=1.0 reward_max=19.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:54,594] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"rewarder.sleep.missed\": {\"calls\": 28, \"std\": \"5.52ms\", \"mean\": \"7.71ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 50, \"std\": \"77.84us\", \"mean\": \"113.73us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 51, \"std\": \"59.20us\", \"mean\": \"73.01us\"}, \"rewarder.sleep\": {\"calls\": 260, \"std\": \"3.60ms\", \"mean\": \"14.78ms\"}, \"rewarder.compute_reward\": {\"calls\": 288, \"std\": \"7.42ms\", \"mean\": \"3.57ms\"}, \"rewarder.frame\": {\"calls\": 288, \"std\": \"2.79ms\", \"mean\": \"17.72ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 51, \"std\": \"91.04us\", \"mean\": \"111.68us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 288, \"std\": \"155.49us\", \"mean\": \"151.16us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 50, \"std\": \"6.82ms\", \"mean\": \"17.18ms\"}, \"reward.parsing.score\": {\"calls\": 51, \"std\": \"7.17ms\", \"mean\": \"17.38ms\"}, \"reward.parsing.gameover\": {\"calls\": 51, \"std\": \"147.93us\", \"mean\": \"306.88us\"}} counters={\"agent_conn.reward\": {\"calls\": 43, \"std\": 9.284836827099834, \"mean\": 10.488372093023258}, \"reward.vnc.updates.n\": {\"calls\": 288, \"std\": 0.38240368512504247, \"mean\": 0.1770833333333334}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 51, \"std\": 117.08903935576139, \"value\": 33230.0, \"mean\": 32926.60784313725}} (export_time=102.76us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:54,616] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=10.2 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=358429.2 vnc_pixels_ps[total]=162631.4 reward_lag=None rewarder_message_lag=None fps=57.62\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:55,151] [INFO:root] [Rewarder] Rewarder fell behind by 0.13078927993774414s from target; losing 7 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:55,415] [INFO:root] [Rewarder] Rewarder fell behind by 0.2137906551361084s from target; losing 12 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:55,952] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.66s, sent 11 reward messages to agent: reward=469.0 reward_min=0.0 reward_max=89.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<2149 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:56,983] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 10 reward messages to agent: reward=48.0 reward_min=1.0 reward_max=18.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:58,033] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.05s, sent 11 reward messages to agent: reward=34.0 reward_min=1.0 reward_max=9.0 done=False info={'rewarder.vnc.updates.bytes': 33721, 'rewarder.vnc.updates.pixels': 11220, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:59,065] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 10 reward messages to agent: reward=105.0 reward_min=1.0 reward_max=28.0 done=False info={'rewarder.vnc.updates.bytes': 36908, 'rewarder.vnc.updates.pixels': 21840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:59,608] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"rewarder.sleep.missed\": {\"calls\": 30, \"std\": \"44.86ms\", \"mean\": \"19.14ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 54, \"std\": \"58.91us\", \"mean\": \"108.10us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 54, \"std\": \"58.03us\", \"mean\": \"77.12us\"}, \"rewarder.sleep\": {\"calls\": 237, \"std\": \"4.11ms\", \"mean\": \"14.15ms\"}, \"rewarder.compute_reward\": {\"calls\": 267, \"std\": \"18.28ms\", \"mean\": \"5.52ms\"}, \"rewarder.frame\": {\"calls\": 267, \"std\": \"16.01ms\", \"mean\": \"19.29ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 54, \"std\": \"53.20us\", \"mean\": \"106.54us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 267, \"std\": \"2.19ms\", \"mean\": \"351.51us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 53, \"std\": \"18.68ms\", \"mean\": \"17.90ms\"}, \"reward.parsing.score\": {\"calls\": 54, \"std\": \"18.71ms\", \"mean\": \"18.05ms\"}, \"reward.parsing.gameover\": {\"calls\": 54, \"std\": \"216.77us\", \"mean\": \"333.58us\"}} counters={\"agent_conn.reward\": {\"calls\": 44, \"std\": 20.94485864344948, \"mean\": 16.31818181818182}, \"reward.vnc.updates.n\": {\"calls\": 267, \"std\": 0.40242989248580924, \"mean\": 0.20224719101123598}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 54, \"std\": 115.98724748572995, \"value\": 33925.0, \"mean\": 33634.42592592591}} (export_time=159.98us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:09:59,626] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=10.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=380077.0 vnc_pixels_ps[total]=171198.8 reward_lag=None rewarder_message_lag=None fps=53.11\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:00,140] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 13 reward messages to agent: reward=212.0 reward_min=0 reward_max=41.0 done=False info={'rewarder.vnc.updates.bytes': 920, 'rewarder.vnc.updates.pixels': 300, 'rewarder.profile': '<2037 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:01,225] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.08s, sent 7 reward messages to agent: reward=10.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:03,741] [INFO:universe.utils] [gameover] Gameover screen detected: distance_n=0.00037073 match_time=432us\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:03,741] [INFO:gym_controlplane.reward.reward] RESET CAUSE: gameover state reached\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:03,742] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.52s, sent 8 reward messages to agent: reward=83.0 reward_min=0.0 reward_max=22.0 done=True info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:03,742] [INFO:root] [Rewarder] Resetting environment since done=True\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:03,742] [INFO:root] [Rewarder] Triggering a reset on EnvController\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:03,744] [INFO:root] [EnvStatus] Changing env_state: running (env_id=flashgames.DuskDrive-v0) -> resetting (env_id=flashgames.DuskDrive-v0) (episode_id: 2->3, fps=60)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:03,744] [INFO:root] [Rewarder] Blocking until env finishes resetting\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:03,745] [INFO:root] [EnvController] controlplane.py is resetting the environment\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:03,745] [INFO:root] [EnvController] Env state: env_id=flashgames.DuskDrive-v0 episode_id=3\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:03,745] [INFO:root] [EnvController] Writing flashgames.DuskDrive-v0 to /tmp/demo/env_id.txt\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:03,746] [ERROR:root] Closing server (via subprocess.close()) and all chromes (via pkill chromedriver || :; pkill chrome || :)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:10:03,771] init detected end of child process 413 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:10:03,781] init detected end of child process 428 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:10:03,794] init detected end of child process 635 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:10:03,824] init detected end of child process 650 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [Wed Jun 28 07:10:03 UTC 2017] [/usr/local/bin/sudoable-env-setup] Allowing outbound network traffic to non-private IPs for git-lfs. (Going to fetch files via git lfs.)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [Wed Jun 28 07:10:03 UTC 2017] [/usr/local/bin/sudoable-env-setup] Completion file /usr/local/openai/git-lfs/flashgames.DuskDrive-v0 exists; not git-lfs pulling\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:10:03,897] init detected end of child process 659 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [Wed Jun 28 07:10:03 UTC 2017] [/usr/local/bin/sudoable-env-setup] Disabling outbound network traffic for flashgames.DuskDrive-v0\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:03,907] [INFO:gym_flashgames.launcher] [EnvController] Launching new Chrome process (attempt 0/10)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:03,907] [INFO:root] Replacing selenium_wrapper_server since we currently do it at every episode boundary\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:10:03,979] init detected end of child process 416 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:10:03,979] init detected end of child process 424 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:10:03,980] init detected end of child process 425 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:10:03,980] init detected end of child process 427 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:04,085] [selenium_wrapper_server] Calling webdriver.Chrome()\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:10:06 [info] 64#64: *33 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:10:06 [info] 64#64: *34 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:10:06 [info] 64#64: *35 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:06,381] [selenium_wrapper_server] Call to webdriver.Chrome() completed: 2.30s\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:06,384] [INFO:gym_flashgames.launcher] [EnvController] Navigating browser to url=http://localhost/flashgames.DuskDrive-v0\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:06,503] [INFO:root] [EnvController] Running command: /app/universe-envs/controlplane/bin/play_vexpect -e flashgames.DuskDrive-v0 -r vnc://127.0.0.1:5900 -d\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:07,688] [play_vexpect] No rewarder addresses were provided, so this env cannot connect to the remote's rewarder channel, and cannot send control messages (e.g. reset)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:07,689] [play_vexpect] Using the golang VNC implementation\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:07,690] [play_vexpect] Using VNCSession arguments: {'start_timeout': 7, 'subsample_level': 2, 'fine_quality_level': 50, 'compress_level': 0, 'encoding': 'zrle'}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:07,691] [play_vexpect] Printed stats will ignore clock skew. (This usually makes sense only when the environment and agent are on the same machine.)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:07,698] [play_vexpect] [0] Connecting to environment: vnc://127.0.0.1:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://None/viewer/?password=openai\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:07,698] [play_vexpect] [0] Connecting to environment details: vnc_address=127.0.0.1:5900 vnc_password=openai rewarder_address=None rewarder_password=openai\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m 2017/06/28 07:10:07 I0628 07:10:07.701071 1175 gymvnc.go:417] [0:127.0.0.1:5900] opening connection to VNC server\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Wed Jun 28 07:10:07 2017\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  Connections: accepted: 127.0.0.1::41078\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  SConnection: Client needs protocol version 3.8\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  SConnection: Client requests security type VncAuth(2)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  VNCSConnST:  Server default pixel format depth 24 (32bpp) little-endian rgb888\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m 2017/06/28 07:10:07 I0628 07:10:07.706095 1175 gymvnc.go:550] [0:127.0.0.1:5900] connection established\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  VNCSConnST:  Client pixel format depth 24 (32bpp) little-endian bgr888\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:08,240] [play_vexpect] Waiting for any of [MaskState<initializing0>] to activate\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:15,340] [play_vexpect] Applying transition: ClickTransition<initializing0->['initializing1'] x=429 y=539 buttonmask=1> for active state MaskState<initializing0>. (Summary: plausible_states=MaskState<initializing0> distance_m=0.0489 match_time_m=240us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:15,356] [play_vexpect] Waiting for any of [MaskState<initializing1>] to activate (or whether any of [MaskState<initializing0>] are still active)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m 2017/06/28 07:10:15 I0628 07:10:15.437767 60 gymvnc.go:374] [0:127.0.0.1:5900] update queue max of 60 reached; pausing further updates\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:15,710] [play_vexpect] Fell behind by 0.13751006126403809s from target; losing 8 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:15,999] [play_vexpect] Fell behind by 0.11861133575439453s from target; losing 7 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:19,435] [play_vexpect] Advancing to the next hopeful state (2/2): MaskState<initializing0>\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:21,467] [play_vexpect] Advancing to the next hopeful state (1/2): MaskState<initializing1>\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:21,852] [play_vexpect] Applying transition: ClickTransition<initializing1->['initializing2'] x=571 y=512 buttonmask=1> for active state MaskState<initializing1>. (Summary: plausible_states=[MaskState<initializing1>, MaskState<initializing0>] distance_m=[0.0044, 0.7509666666666667] match_time_m=['273us', '452us'])\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:21,868] [play_vexpect] Waiting for any of [MaskState<initializing2>] to activate (or whether any of [MaskState<initializing1>] are still active)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:22,351] [play_vexpect] Applying transition: ClickTransition<initializing2->['ready0', 'ready1', 'ready2', 'ready3'] x=216 y=296 buttonmask=1> for active state MaskState<initializing2>. (Summary: plausible_states=[MaskState<initializing2>, MaskState<initializing1>] distance_m=[0.0359, 0.9961] match_time_m=['226us', '190us'])\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:22,367] [play_vexpect] Waiting for any of [ready0, ready1, ready2, ready3] to activate (or whether any of [MaskState<initializing2>] are still active)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:22,935] [play_vexpect] Applying transition: ClickTransition<ready1->[] x=0 y=0 buttonmask=0> for active state ready1. (Summary: plausible_states=[ready0, ready1, ready2, ready3, MaskState<initializing2>] distance_m=[0.31458586, 0.14552443, 0.16938278, 0.01057476, 0.997] match_time_m=['214us', '101us', '79us', '73us', '179us'])\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:22,939] [play_vexpect] Reaching start state: ready1\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:22,940] [play_vexpect] vexpect macro complete in 15.199505s\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Wed Jun 28 07:10:23 2017\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  Connections: closed: 127.0.0.1::41078 (Clean disconnection)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager: Framebuffer updates: 198\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:   ZRLE:\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Solid: 25 rects, 834.065 kpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:            1.95508 KiB (1:1666.61 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Bitmap RLE: 80 rects, 1.23639 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:                 160.924 KiB (1:30.0177 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Indexed RLE: 129 rects, 593.766 kpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:                  164.745 KiB (1:14.0879 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Full Colour: 186 rects, 6.9002 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:                  19.751 MiB (1:1.33281 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:   Total: 420 rects, 9.56442 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:          20.071 MiB (1:1.81805 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:23,402] [INFO:root] [EnvStatus] Changing env_state: resetting (env_id=flashgames.DuskDrive-v0) -> running (env_id=flashgames.DuskDrive-v0) (episode_id: 3->3, fps=60)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:23,405] [INFO:root] [Rewarder] Unblocking since env reset finished\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:23,409] [INFO:root] [Rewarder] Clearing reward_parser state: env_id=flashgames.DuskDrive-v0 episode_id=2->3, env_state=running\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:23,416] [INFO:universe.rewarder.remote] [Rewarder] Over past 19.67s, sent 0 reward messages to agent: reward=0 reward_min=(empty) reward_max=(empty) done=False info={}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:23,417] [INFO:universe.rewarder.remote] [Rewarder] Ending previous episode: episode_reward=22371.0 episode_count=572 episode_duration=94.48\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m 2017/06/28 07:10:23 I0628 07:10:23.41982 60 gymvnc.go:278] [0:127.0.0.1:5900] resuming updates\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:23,429] [INFO:universe.wrappers.logger] Stats for the past 23.80s: vnc_updates_ps=2.0 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=63317.6 vnc_pixels_ps[total]=30517.3 reward_lag=None rewarder_message_lag=None fps=9.87\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:23,456] [INFO:universe.pyprofile] [pyprofile] period=23.85s timers={\"rewarder.sleep.missed\": {\"calls\": 25, \"std\": \"5.70ms\", \"mean\": \"8.18ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 44, \"std\": \"79.15us\", \"mean\": \"118.44us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 49, \"std\": \"35.29us\", \"mean\": \"67.48us\"}, \"rewarder.sleep\": {\"calls\": 210, \"std\": \"3.43ms\", \"mean\": \"15.00ms\"}, \"rewarder.compute_reward\": {\"calls\": 236, \"std\": \"8.31ms\", \"mean\": \"3.97ms\"}, \"rewarder.frame\": {\"calls\": 236, \"std\": \"3.06ms\", \"mean\": \"17.78ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 49, \"std\": \"42.50us\", \"mean\": \"93.16us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 236, \"std\": \"106.89us\", \"mean\": \"136.95us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 43, \"std\": \"6.87ms\", \"mean\": \"18.49ms\"}, \"reward.parsing.score\": {\"calls\": 49, \"std\": \"8.95ms\", \"mean\": \"16.74ms\"}, \"reward.parsing.gameover\": {\"calls\": 49, \"std\": \"188.79us\", \"mean\": \"299.82us\"}} counters={\"agent_conn.done\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}, \"agent_conn.reward\": {\"calls\": 22, \"std\": 6.38924962906528, \"mean\": 5.181818181818182}, \"reward.vnc.updates.n\": {\"calls\": 236, \"std\": 3.977948139784233, \"mean\": 0.4618644067796611}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 5, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 6, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 48, \"std\": 45.27379277304479, \"value\": 34062.0, \"mean\": 34012.666666666664}} (export_time=248.91us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:23,489] [INFO:gym_controlplane.reward.reward] First score parsed: score=695\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:24,503] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.09s, sent 9 reward messages to agent: reward=1134.0 reward_min=-26.0 reward_max=284.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.profile': '<2283 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:25,571] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 11 reward messages to agent: reward=2526.0 reward_min=129.0 reward_max=348.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:26,724] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.15s, sent 3 reward messages to agent: reward=1260.0 reward_min=371.0 reward_max=477.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:27,820] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.10s, sent 3 reward messages to agent: reward=1539.0 reward_min=434.0 reward_max=591.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:28,443] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=18.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=243103.1 vnc_pixels_ps[total]=204619.9 reward_lag=None rewarder_message_lag=None fps=55.27\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:28,460] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"rewarder.sleep.missed\": {\"calls\": 32, \"std\": \"14.24ms\", \"mean\": \"12.96ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 30, \"std\": \"186.43us\", \"mean\": \"148.14us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 31, \"std\": \"117.41us\", \"mean\": \"99.51us\"}, \"rewarder.sleep\": {\"calls\": 246, \"std\": \"2.62ms\", \"mean\": \"14.98ms\"}, \"rewarder.compute_reward\": {\"calls\": 278, \"std\": \"9.70ms\", \"mean\": \"3.76ms\"}, \"rewarder.frame\": {\"calls\": 278, \"std\": \"6.47ms\", \"mean\": \"18.91ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 31, \"std\": \"123.32us\", \"mean\": \"143.58us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 278, \"std\": \"648.85us\", \"mean\": \"274.38us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 29, \"std\": \"8.14ms\", \"mean\": \"23.05ms\"}, \"reward.parsing.score\": {\"calls\": 31, \"std\": \"9.78ms\", \"mean\": \"22.13ms\"}, \"reward.parsing.gameover\": {\"calls\": 31, \"std\": \"692.11us\", \"mean\": \"512.30us\"}} counters={\"agent_conn.reward\": {\"calls\": 29, \"std\": 164.09079998706233, \"mean\": 237.17241379310343}, \"reward.vnc.updates.n\": {\"calls\": 278, \"std\": 0.3308262643646443, \"mean\": 0.11510791366906473}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 2, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 30, \"std\": 2248.88975443133, \"value\": 7572.0, \"mean\": 3177.9}} (export_time=542.40us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:28,873] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.05s, sent 7 reward messages to agent: reward=432.0 reward_min=0 reward_max=407.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.profile': '<2173 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:29,900] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 9 reward messages to agent: reward=58.0 reward_min=1.0 reward_max=25.0 done=False info={'rewarder.vnc.updates.bytes': 35041, 'rewarder.vnc.updates.pixels': 21020, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:30,935] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 10 reward messages to agent: reward=23.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:31,956] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 12 reward messages to agent: reward=23.0 reward_min=1.0 reward_max=7.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:32,998] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 9 reward messages to agent: reward=16.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:33,459] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=14.2 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=479622.1 vnc_pixels_ps[total]=197073.1 reward_lag=None rewarder_message_lag=None fps=58.84\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:33,477] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"rewarder.sleep.missed\": {\"calls\": 22, \"std\": \"3.41ms\", \"mean\": \"4.50ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 68, \"std\": \"92.64us\", \"mean\": \"120.25us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 71, \"std\": \"43.82us\", \"mean\": \"72.90us\"}, \"rewarder.sleep\": {\"calls\": 272, \"std\": \"4.78ms\", \"mean\": \"13.73ms\"}, \"rewarder.compute_reward\": {\"calls\": 294, \"std\": \"6.56ms\", \"mean\": \"3.83ms\"}, \"rewarder.frame\": {\"calls\": 294, \"std\": \"1.48ms\", \"mean\": \"17.32ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 71, \"std\": \"62.67us\", \"mean\": \"111.01us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 294, \"std\": \"147.41us\", \"mean\": \"152.82us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 67, \"std\": \"3.93ms\", \"mean\": \"14.00ms\"}, \"reward.parsing.score\": {\"calls\": 71, \"std\": \"4.99ms\", \"mean\": \"13.73ms\"}, \"reward.parsing.gameover\": {\"calls\": 71, \"std\": \"229.68us\", \"mean\": \"323.90us\"}} counters={\"agent_conn.reward\": {\"calls\": 50, \"std\": 4.100821221886848, \"mean\": 2.8600000000000008}, \"reward.vnc.updates.n\": {\"calls\": 294, \"std\": 0.4386114723554637, \"mean\": 0.24489795918367344}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 3, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 4, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 71, \"std\": 34.230441697895294, \"value\": 7716.0, \"mean\": 7669.8169014084515}} (export_time=314.24us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:34,062] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 12 reward messages to agent: reward=19.0 reward_min=0.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<2193 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:35,125] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 9 reward messages to agent: reward=19.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:36,195] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 8 reward messages to agent: reward=17.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:36,226] [INFO:root] [Rewarder] Rewarder fell behind by 0.20771431922912598s from target; losing 12 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:36,398] [INFO:root] [Rewarder] Rewarder fell behind by 0.12064313888549805s from target; losing 7 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:37,485] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.26s, sent 2 reward messages to agent: reward=4.0 reward_min=2.0 reward_max=2.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:38,473] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=8.2 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=284804.2 vnc_pixels_ps[total]=135718.1 reward_lag=None rewarder_message_lag=None fps=54.07\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:38,488] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"rewarder.sleep.missed\": {\"calls\": 21, \"std\": \"51.82ms\", \"mean\": \"23.61ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 41, \"std\": \"230.40us\", \"mean\": \"138.47us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 41, \"std\": \"101.00us\", \"mean\": \"88.79us\"}, \"rewarder.sleep\": {\"calls\": 250, \"std\": \"3.95ms\", \"mean\": \"14.37ms\"}, \"rewarder.compute_reward\": {\"calls\": 271, \"std\": \"13.92ms\", \"mean\": \"4.03ms\"}, \"rewarder.frame\": {\"calls\": 271, \"std\": \"15.59ms\", \"mean\": \"18.99ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 41, \"std\": \"99.00us\", \"mean\": \"134.58us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 271, \"std\": \"1.69ms\", \"mean\": \"343.65us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 40, \"std\": \"26.73ms\", \"mean\": \"18.63ms\"}, \"reward.parsing.score\": {\"calls\": 41, \"std\": \"26.78ms\", \"mean\": \"18.77ms\"}, \"reward.parsing.gameover\": {\"calls\": 41, \"std\": \"917.49us\", \"mean\": \"447.98us\"}} counters={\"agent_conn.reward\": {\"calls\": 30, \"std\": 2.490441497119479, \"mean\": 2.733333333333334}, \"reward.vnc.updates.n\": {\"calls\": 271, \"std\": 0.3589957596493601, \"mean\": 0.1512915129151292}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 41, \"std\": 20.97763036212032, \"value\": 7798.0, \"mean\": 7745.804878048781}} (export_time=104.67us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:38,490] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 6 reward messages to agent: reward=35.0 reward_min=2.0 reward_max=11.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<2039 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:39,551] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 6 reward messages to agent: reward=44.0 reward_min=1.0 reward_max=15.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:40,562] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.01s, sent 13 reward messages to agent: reward=794.0 reward_min=20.0 reward_max=110.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:41,730] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.17s, sent 13 reward messages to agent: reward=372.0 reward_min=1.0 reward_max=119.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:42,845] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.11s, sent 9 reward messages to agent: reward=22.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 33721, 'rewarder.vnc.updates.pixels': 11220, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:43,481] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=12.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=429935.8 vnc_pixels_ps[total]=192996.9 reward_lag=None rewarder_message_lag=None fps=58.72\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:43,508] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"rewarder.sleep.missed\": {\"calls\": 24, \"std\": \"4.03ms\", \"mean\": \"4.49ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 61, \"std\": \"101.63us\", \"mean\": \"109.95us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 64, \"std\": \"20.87us\", \"mean\": \"57.47us\"}, \"rewarder.sleep\": {\"calls\": 270, \"std\": \"4.34ms\", \"mean\": \"14.16ms\"}, \"rewarder.compute_reward\": {\"calls\": 294, \"std\": \"6.70ms\", \"mean\": \"3.62ms\"}, \"rewarder.frame\": {\"calls\": 294, \"std\": \"1.64ms\", \"mean\": \"17.31ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 64, \"std\": \"115.19us\", \"mean\": \"144.13us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 294, \"std\": \"141.66us\", \"mean\": \"155.95us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 60, \"std\": \"4.78ms\", \"mean\": \"14.67ms\"}, \"reward.parsing.score\": {\"calls\": 64, \"std\": \"5.89ms\", \"mean\": \"14.26ms\"}, \"reward.parsing.gameover\": {\"calls\": 64, \"std\": \"158.61us\", \"mean\": \"272.23us\"}} counters={\"agent_conn.reward\": {\"calls\": 48, \"std\": 33.32399913635391, \"mean\": 26.020833333333336}, \"reward.vnc.updates.n\": {\"calls\": 294, \"std\": 0.4133771146846397, \"mean\": 0.21768707482993213}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 3, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 4, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 64, \"std\": 493.27070845072126, \"value\": 9047.0, \"mean\": 8664.765624999998}} (export_time=104.19us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:43,970] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.12s, sent 9 reward messages to agent: reward=20.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.profile': '<2194 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:45,032] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 10 reward messages to agent: reward=23.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 33721, 'rewarder.vnc.updates.pixels': 11220, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:46,209] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.18s, sent 11 reward messages to agent: reward=20.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:46,399] [INFO:root] [Rewarder] Rewarder fell behind by 0.12226653099060059s from target; losing 7 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:47,580] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.37s, sent 3 reward messages to agent: reward=5.0 reward_min=1.0 reward_max=2.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:48,488] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=11.4 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=370622.0 vnc_pixels_ps[total]=165821.1 reward_lag=None rewarder_message_lag=None fps=54.33\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:48,524] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"rewarder.sleep.missed\": {\"calls\": 29, \"std\": \"24.89ms\", \"mean\": \"11.81ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 53, \"std\": \"45.28us\", \"mean\": \"98.64us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 56, \"std\": \"80.72us\", \"mean\": \"76.59us\"}, \"rewarder.sleep\": {\"calls\": 245, \"std\": \"4.40ms\", \"mean\": \"14.07ms\"}, \"rewarder.compute_reward\": {\"calls\": 274, \"std\": \"8.11ms\", \"mean\": \"3.88ms\"}, \"rewarder.frame\": {\"calls\": 274, \"std\": \"18.16ms\", \"mean\": \"19.50ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 56, \"std\": \"119.58us\", \"mean\": \"135.40us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 274, \"std\": \"442.20us\", \"mean\": \"184.93us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 52, \"std\": \"4.19ms\", \"mean\": \"14.09ms\"}, \"reward.parsing.score\": {\"calls\": 56, \"std\": \"5.51ms\", \"mean\": \"13.57ms\"}, \"reward.parsing.gameover\": {\"calls\": 56, \"std\": \"198.83us\", \"mean\": \"288.24us\"}} counters={\"agent_conn.reward\": {\"calls\": 34, \"std\": 2.0179674212853347, \"mean\": 2.558823529411764}, \"reward.vnc.updates.n\": {\"calls\": 274, \"std\": 0.41555089771886833, \"mean\": 0.2080291970802917}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 3, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 4, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 56, \"std\": 20.34418133731814, \"value\": 9132.0, \"mean\": 9082.428571428574}} (export_time=344.51us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:48,680] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.10s, sent 10 reward messages to agent: reward=36.0 reward_min=0 reward_max=10.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<2183 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:50,059] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.38s, sent 10 reward messages to agent: reward=23.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 33721, 'rewarder.vnc.updates.pixels': 11220, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:51,133] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 9 reward messages to agent: reward=20.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 35041, 'rewarder.vnc.updates.pixels': 21020, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:52,185] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.05s, sent 8 reward messages to agent: reward=22.0 reward_min=1.0 reward_max=8.0 done=False info={'rewarder.vnc.updates.bytes': 33721, 'rewarder.vnc.updates.pixels': 11220, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:53,379] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.19s, sent 10 reward messages to agent: reward=21.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 35041, 'rewarder.vnc.updates.pixels': 21020, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:53,491] [INFO:universe.wrappers.logger] Stats for the past 5.00s: vnc_updates_ps=13.4 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=467788.3 vnc_pixels_ps[total]=210064.1 reward_lag=None rewarder_message_lag=None fps=58.79\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:53,540] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"rewarder.sleep.missed\": {\"calls\": 26, \"std\": \"2.75ms\", \"mean\": \"3.77ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 65, \"std\": \"139.90us\", \"mean\": \"117.98us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 66, \"std\": \"58.29us\", \"mean\": \"70.77us\"}, \"rewarder.sleep\": {\"calls\": 269, \"std\": \"4.63ms\", \"mean\": \"14.02ms\"}, \"rewarder.compute_reward\": {\"calls\": 295, \"std\": \"6.72ms\", \"mean\": \"3.82ms\"}, \"rewarder.frame\": {\"calls\": 295, \"std\": \"1.30ms\", \"mean\": \"17.26ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 66, \"std\": \"65.14us\", \"mean\": \"108.47us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 295, \"std\": \"91.31us\", \"mean\": \"136.24us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 65, \"std\": \"3.99ms\", \"mean\": \"14.74ms\"}, \"reward.parsing.score\": {\"calls\": 66, \"std\": \"4.33ms\", \"mean\": \"14.99ms\"}, \"reward.parsing.gameover\": {\"calls\": 66, \"std\": \"228.11us\", \"mean\": \"305.40us\"}} counters={\"agent_conn.reward\": {\"calls\": 41, \"std\": 1.7306420632873545, \"mean\": 2.1707317073170733}, \"reward.vnc.updates.n\": {\"calls\": 295, \"std\": 0.41745054823461036, \"mean\": 0.22372881355932203}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 66, \"std\": 28.210877566170442, \"value\": 9222.0, \"mean\": 9179.515151515157}} (export_time=136.14us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:54,454] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 9 reward messages to agent: reward=21.0 reward_min=0 reward_max=7.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<2150 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:55,539] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.09s, sent 8 reward messages to agent: reward=22.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:57,173] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.63s, sent 7 reward messages to agent: reward=20.0 reward_min=1.0 reward_max=7.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:57,189] [INFO:root] [Rewarder] Rewarder fell behind by 0.20461416244506836s from target; losing 12 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:58,238] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.05s, sent 3 reward messages to agent: reward=5.0 reward_min=1.0 reward_max=2.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:58,503] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=6.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=238489.2 vnc_pixels_ps[total]=114895.3 reward_lag=None rewarder_message_lag=None fps=53.89\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:58,552] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"rewarder.sleep.missed\": {\"calls\": 25, \"std\": \"42.35ms\", \"mean\": \"19.57ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 34, \"std\": \"416.91us\", \"mean\": \"160.83us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 35, \"std\": \"656.36us\", \"mean\": \"204.89us\"}, \"rewarder.sleep\": {\"calls\": 245, \"std\": \"3.35ms\", \"mean\": \"14.96ms\"}, \"rewarder.compute_reward\": {\"calls\": 270, \"std\": \"14.27ms\", \"mean\": \"3.84ms\"}, \"rewarder.frame\": {\"calls\": 270, \"std\": \"15.11ms\", \"mean\": \"19.22ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 35, \"std\": \"89.07us\", \"mean\": \"111.17us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 270, \"std\": \"3.72ms\", \"mean\": \"421.55us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 34, \"std\": \"9.61ms\", \"mean\": \"17.65ms\"}, \"reward.parsing.score\": {\"calls\": 35, \"std\": \"10.15ms\", \"mean\": \"17.81ms\"}, \"reward.parsing.gameover\": {\"calls\": 35, \"std\": \"21.23ms\", \"mean\": \"4.16ms\"}} counters={\"agent_conn.reward\": {\"calls\": 28, \"std\": 2.9938207967349952, \"mean\": 3.0}, \"reward.vnc.updates.n\": {\"calls\": 270, \"std\": 0.3365192646427523, \"mean\": 0.12962962962962954}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 35, \"std\": 21.091636798910667, \"value\": 9305.0, \"mean\": 9261.714285714284}} (export_time=268.46us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:10:59,376] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.14s, sent 9 reward messages to agent: reward=37.0 reward_min=0 reward_max=15.0 done=False info={'rewarder.vnc.updates.bytes': 35041, 'rewarder.vnc.updates.pixels': 21020, 'rewarder.profile': '<2135 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:00,428] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.05s, sent 11 reward messages to agent: reward=124.0 reward_min=1.0 reward_max=27.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:01,502] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 11 reward messages to agent: reward=641.0 reward_min=8.0 reward_max=193.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:02,565] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 10 reward messages to agent: reward=25.0 reward_min=1.0 reward_max=7.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:03,512] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=10.4 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=374262.4 vnc_pixels_ps[total]=173421.0 reward_lag=None rewarder_message_lag=None fps=58.70\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:03,582] [INFO:universe.pyprofile] [pyprofile] period=5.03s timers={\"rewarder.sleep.missed\": {\"calls\": 23, \"std\": \"5.41ms\", \"mean\": \"4.74ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 52, \"std\": \"167.39us\", \"mean\": \"110.54us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 52, \"std\": \"27.77us\", \"mean\": \"59.47us\"}, \"rewarder.sleep\": {\"calls\": 271, \"std\": \"3.85ms\", \"mean\": \"14.56ms\"}, \"rewarder.compute_reward\": {\"calls\": 294, \"std\": \"6.52ms\", \"mean\": \"3.21ms\"}, \"rewarder.frame\": {\"calls\": 294, \"std\": \"1.93ms\", \"mean\": \"17.35ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 52, \"std\": \"250.10us\", \"mean\": \"150.76us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 294, \"std\": \"238.32us\", \"mean\": \"138.53us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 52, \"std\": \"5.49ms\", \"mean\": \"14.96ms\"}, \"reward.parsing.score\": {\"calls\": 52, \"std\": \"5.51ms\", \"mean\": \"15.49ms\"}, \"reward.parsing.gameover\": {\"calls\": 52, \"std\": \"299.62us\", \"mean\": \"294.99us\"}} counters={\"agent_conn.reward\": {\"calls\": 46, \"std\": 34.589546325094304, \"mean\": 17.913043478260875}, \"reward.vnc.updates.n\": {\"calls\": 294, \"std\": 0.3822098537655448, \"mean\": 0.1768707482993199}} gauges={\"reward_parser.score.last_score\": {\"calls\": 52, \"std\": 362.8689992731323, \"value\": 10131.0, \"mean\": 9775.173076923076}} (export_time=197.41us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:03,583] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 8 reward messages to agent: reward=15.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<1934 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:04,617] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 9 reward messages to agent: reward=23.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:05,643] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 9 reward messages to agent: reward=20.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 35067, 'rewarder.vnc.updates.pixels': 21220, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:11:06 [info] 64#64: *37 client timed out (110: Connection timed out) while waiting for request, client: 127.0.0.1, server: 0.0.0.0:80\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:11:06 [info] 64#64: *38 client timed out (110: Connection timed out) while waiting for request, client: 127.0.0.1, server: 0.0.0.0:80\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:11:06 [info] 64#64: *39 client timed out (110: Connection timed out) while waiting for request, client: 127.0.0.1, server: 0.0.0.0:80\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:11:06 [info] 64#64: *40 client timed out (110: Connection timed out) while waiting for request, client: 127.0.0.1, server: 0.0.0.0:80\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:06,740] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.10s, sent 8 reward messages to agent: reward=19.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:07,545] [INFO:root] [Rewarder] Rewarder fell behind by 0.11020970344543457s from target; losing 6 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:07,829] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.09s, sent 1 reward messages to agent: reward=1.0 reward_min=1.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:08,543] [INFO:universe.wrappers.logger] Stats for the past 5.03s: vnc_updates_ps=8.0 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=285192.6 vnc_pixels_ps[total]=132430.6 reward_lag=None rewarder_message_lag=None fps=52.89\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:08,593] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"rewarder.sleep.missed\": {\"calls\": 38, \"std\": \"19.84ms\", \"mean\": \"15.68ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 39, \"std\": \"92.34us\", \"mean\": \"122.83us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 39, \"std\": \"42.74us\", \"mean\": \"77.78us\"}, \"rewarder.sleep\": {\"calls\": 228, \"std\": \"3.29ms\", \"mean\": \"14.87ms\"}, \"rewarder.compute_reward\": {\"calls\": 266, \"std\": \"12.26ms\", \"mean\": \"5.13ms\"}, \"rewarder.frame\": {\"calls\": 266, \"std\": \"9.46ms\", \"mean\": \"19.55ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 39, \"std\": \"102.57us\", \"mean\": \"121.59us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 266, \"std\": \"4.59ms\", \"mean\": \"659.21us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 39, \"std\": \"7.68ms\", \"mean\": \"18.81ms\"}, \"reward.parsing.score\": {\"calls\": 39, \"std\": \"7.74ms\", \"mean\": \"19.34ms\"}, \"reward.parsing.gameover\": {\"calls\": 39, \"std\": \"148.74us\", \"mean\": \"326.44us\"}} counters={\"agent_conn.reward\": {\"calls\": 30, \"std\": 1.5698305055298023, \"mean\": 2.466666666666667}, \"reward.vnc.updates.n\": {\"calls\": 266, \"std\": 0.3543900148047003, \"mean\": 0.14661654135338353}} gauges={\"reward_parser.score.last_score\": {\"calls\": 39, \"std\": 19.268715598339377, \"value\": 10199.0, \"mean\": 10168.923076923076}} (export_time=235.32us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:08,866] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 5 reward messages to agent: reward=21.0 reward_min=0 reward_max=8.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.profile': '<1932 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:09,872] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.01s, sent 7 reward messages to agent: reward=27.0 reward_min=1.0 reward_max=8.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:10,918] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.05s, sent 11 reward messages to agent: reward=23.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:11,930] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.01s, sent 11 reward messages to agent: reward=24.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:13,003] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 9 reward messages to agent: reward=14.0 reward_min=1.0 reward_max=3.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:13,555] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=10.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=373072.0 vnc_pixels_ps[total]=171166.4 reward_lag=None rewarder_message_lag=None fps=55.88\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:13,609] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"rewarder.sleep.missed\": {\"calls\": 41, \"std\": \"4.55ms\", \"mean\": \"8.41ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 53, \"std\": \"112.06us\", \"mean\": \"118.25us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 54, \"std\": \"52.21us\", \"mean\": \"70.66us\"}, \"rewarder.sleep\": {\"calls\": 238, \"std\": \"2.71ms\", \"mean\": \"15.38ms\"}, \"rewarder.compute_reward\": {\"calls\": 279, \"std\": \"8.88ms\", \"mean\": \"4.44ms\"}, \"rewarder.frame\": {\"calls\": 279, \"std\": \"3.37ms\", \"mean\": \"18.18ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 54, \"std\": \"77.99us\", \"mean\": \"105.91us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 279, \"std\": \"132.59us\", \"mean\": \"136.26us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 53, \"std\": \"6.05ms\", \"mean\": \"20.56ms\"}, \"reward.parsing.score\": {\"calls\": 54, \"std\": \"6.68ms\", \"mean\": \"20.68ms\"}, \"reward.parsing.gameover\": {\"calls\": 54, \"std\": \"267.70us\", \"mean\": \"333.98us\"}} counters={\"agent_conn.reward\": {\"calls\": 44, \"std\": 1.8861630347500467, \"mean\": 2.522727272727273}, \"reward.vnc.updates.n\": {\"calls\": 279, \"std\": 0.3957889266374541, \"mean\": 0.19354838709677413}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 54, \"std\": 27.671831499927336, \"value\": 10316.0, \"mean\": 10267.925925925927}} (export_time=175.24us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:14,145] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.14s, sent 6 reward messages to agent: reward=17.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<2157 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:15,153] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 6 reward messages to agent: reward=25.0 reward_min=1.0 reward_max=7.0 done=False info={'rewarder.vnc.updates.bytes': 35067, 'rewarder.vnc.updates.pixels': 21220, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:16,195] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 10 reward messages to agent: reward=23.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 33721, 'rewarder.vnc.updates.pixels': 11220, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:17,942] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.75s, sent 7 reward messages to agent: reward=14.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:18,562] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=6.4 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=228266.9 vnc_pixels_ps[total]=106096.3 reward_lag=None rewarder_message_lag=None fps=57.73\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:18,612] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"rewarder.sleep.missed\": {\"calls\": 25, \"std\": \"5.83ms\", \"mean\": \"7.57ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 31, \"std\": \"28.40us\", \"mean\": \"96.15us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 31, \"std\": \"21.90us\", \"mean\": \"60.23us\"}, \"rewarder.sleep\": {\"calls\": 265, \"std\": \"3.62ms\", \"mean\": \"14.62ms\"}, \"rewarder.compute_reward\": {\"calls\": 290, \"std\": \"5.67ms\", \"mean\": \"2.61ms\"}, \"rewarder.frame\": {\"calls\": 290, \"std\": \"4.44ms\", \"mean\": \"18.25ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 31, \"std\": \"36.88us\", \"mean\": \"91.32us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 290, \"std\": \"2.35ms\", \"mean\": \"479.21us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 31, \"std\": \"3.99ms\", \"mean\": \"14.18ms\"}, \"reward.parsing.score\": {\"calls\": 31, \"std\": \"4.00ms\", \"mean\": \"14.60ms\"}, \"reward.parsing.gameover\": {\"calls\": 31, \"std\": \"76.11us\", \"mean\": \"257.48us\"}} counters={\"agent_conn.reward\": {\"calls\": 27, \"std\": 1.8265219232690137, \"mean\": 2.5185185185185186}, \"reward.vnc.updates.n\": {\"calls\": 290, \"std\": 0.30951578936057644, \"mean\": 0.10689655172413791}} gauges={\"reward_parser.score.last_score\": {\"calls\": 31, \"std\": 21.320543881380438, \"value\": 10383.0, \"mean\": 10355.967741935483}} (export_time=359.06us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:18,989] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.05s, sent 4 reward messages to agent: reward=17.0 reward_min=0 reward_max=14.0 done=False info={'rewarder.vnc.updates.bytes': 36908, 'rewarder.vnc.updates.pixels': 21840, 'rewarder.profile': '<1930 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:20,007] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 6 reward messages to agent: reward=27.0 reward_min=1.0 reward_max=12.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:21,042] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 7 reward messages to agent: reward=20.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 36908, 'rewarder.vnc.updates.pixels': 21840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:22,112] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 7 reward messages to agent: reward=22.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:23,150] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 7 reward messages to agent: reward=22.0 reward_min=1.0 reward_max=7.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:23,566] [INFO:universe.wrappers.logger] Stats for the past 5.00s: vnc_updates_ps=7.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=274857.3 vnc_pixels_ps[total]=132937.7 reward_lag=None rewarder_message_lag=None fps=59.37\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:23,638] [INFO:universe.pyprofile] [pyprofile] period=5.03s timers={\"rewarder.sleep.missed\": {\"calls\": 13, \"std\": \"2.54ms\", \"mean\": \"4.09ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 39, \"std\": \"26.94us\", \"mean\": \"93.51us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 39, \"std\": \"20.21us\", \"mean\": \"62.50us\"}, \"rewarder.sleep\": {\"calls\": 284, \"std\": \"3.58ms\", \"mean\": \"14.78ms\"}, \"rewarder.compute_reward\": {\"calls\": 297, \"std\": \"5.28ms\", \"mean\": \"2.35ms\"}, \"rewarder.frame\": {\"calls\": 297, \"std\": \"982.73us\", \"mean\": \"17.13ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 39, \"std\": \"51.10us\", \"mean\": \"103.51us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 297, \"std\": \"146.28us\", \"mean\": \"137.74us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 39, \"std\": \"4.09ms\", \"mean\": \"14.14ms\"}, \"reward.parsing.score\": {\"calls\": 39, \"std\": \"4.12ms\", \"mean\": \"14.62ms\"}, \"reward.parsing.gameover\": {\"calls\": 39, \"std\": \"99.63us\", \"mean\": \"267.88us\"}} counters={\"agent_conn.reward\": {\"calls\": 33, \"std\": 2.9164502084180723, \"mean\": 3.4545454545454546}, \"reward.vnc.updates.n\": {\"calls\": 297, \"std\": 0.3383125248278054, \"mean\": 0.1313131313131312}} gauges={\"reward_parser.score.last_score\": {\"calls\": 39, \"std\": 31.56126360467845, \"value\": 10498.0, \"mean\": 10453.692307692305}} (export_time=203.13us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:24,271] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.12s, sent 7 reward messages to agent: reward=18.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<1939 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:25,306] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 7 reward messages to agent: reward=23.0 reward_min=1.0 reward_max=7.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:26,427] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.12s, sent 7 reward messages to agent: reward=21.0 reward_min=1.0 reward_max=7.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:28,124] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.70s, sent 6 reward messages to agent: reward=20.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 21664, 'rewarder.vnc.updates.pixels': 16000, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:28,582] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=6.2 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=215601.7 vnc_pixels_ps[total]=99776.6 reward_lag=None rewarder_message_lag=None fps=56.83\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:28,648] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"rewarder.sleep.missed\": {\"calls\": 35, \"std\": \"5.19ms\", \"mean\": \"7.58ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 30, \"std\": \"201.72us\", \"mean\": \"133.90us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 30, \"std\": \"473.66us\", \"mean\": \"182.91us\"}, \"rewarder.sleep\": {\"calls\": 251, \"std\": \"2.26ms\", \"mean\": \"15.46ms\"}, \"rewarder.compute_reward\": {\"calls\": 286, \"std\": \"6.29ms\", \"mean\": \"2.69ms\"}, \"rewarder.frame\": {\"calls\": 286, \"std\": \"4.89ms\", \"mean\": \"18.52ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 30, \"std\": \"46.34us\", \"mean\": \"106.26us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 286, \"std\": \"739.74us\", \"mean\": \"219.68us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 29, \"std\": \"4.70ms\", \"mean\": \"17.94ms\"}, \"reward.parsing.score\": {\"calls\": 30, \"std\": \"5.59ms\", \"mean\": \"17.83ms\"}, \"reward.parsing.gameover\": {\"calls\": 30, \"std\": \"1.02ms\", \"mean\": \"496.01us\"}} counters={\"agent_conn.reward\": {\"calls\": 24, \"std\": 2.0496553262736, \"mean\": 3.1249999999999996}, \"reward.vnc.updates.n\": {\"calls\": 286, \"std\": 0.30695531907067264, \"mean\": 0.10489510489510499}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 29, \"std\": 21.292821242220864, \"value\": 10572.0, \"mean\": 10536.793103448277}} (export_time=226.02us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:29,161] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 3 reward messages to agent: reward=13.0 reward_min=0 reward_max=11.0 done=False info={'rewarder.vnc.updates.bytes': 36908, 'rewarder.vnc.updates.pixels': 21840, 'rewarder.profile': '<2046 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:30,208] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.05s, sent 10 reward messages to agent: reward=1998.0 reward_min=79.0 reward_max=305.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:31,277] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 12 reward messages to agent: reward=1608.0 reward_min=19.0 reward_max=299.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:32,299] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 7 reward messages to agent: reward=33.0 reward_min=1.0 reward_max=17.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:33,363] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 8 reward messages to agent: reward=33.0 reward_min=1.0 reward_max=8.0 done=False info={'rewarder.vnc.updates.bytes': 36908, 'rewarder.vnc.updates.pixels': 21840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:33,590] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=8.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=317798.6 vnc_pixels_ps[total]=151242.8 reward_lag=None rewarder_message_lag=None fps=57.32\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:33,659] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"rewarder.sleep.missed\": {\"calls\": 29, \"std\": \"5.13ms\", \"mean\": \"7.82ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 45, \"std\": \"43.50us\", \"mean\": \"94.74us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 45, \"std\": \"50.04us\", \"mean\": \"70.71us\"}, \"rewarder.sleep\": {\"calls\": 258, \"std\": \"3.17ms\", \"mean\": \"15.12ms\"}, \"rewarder.compute_reward\": {\"calls\": 287, \"std\": \"7.59ms\", \"mean\": \"3.43ms\"}, \"rewarder.frame\": {\"calls\": 287, \"std\": \"2.79ms\", \"mean\": \"17.76ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 45, \"std\": \"119.24us\", \"mean\": \"131.72us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 287, \"std\": \"134.79us\", \"mean\": \"143.73us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 45, \"std\": \"6.43ms\", \"mean\": \"18.58ms\"}, \"reward.parsing.score\": {\"calls\": 45, \"std\": \"6.45ms\", \"mean\": \"19.13ms\"}, \"reward.parsing.gameover\": {\"calls\": 45, \"std\": \"313.42us\", \"mean\": \"310.67us\"}} counters={\"agent_conn.reward\": {\"calls\": 42, \"std\": 105.11269600666049, \"mean\": 87.83333333333336}, \"reward.vnc.updates.n\": {\"calls\": 287, \"std\": 0.3768992821462217, \"mean\": 0.16027874564459932}} gauges={\"reward_parser.score.last_score\": {\"calls\": 45, \"std\": 1382.3153055798825, \"value\": 14259.0, \"mean\": 13188.177777777777}} (export_time=148.53us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:34,473] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.11s, sent 8 reward messages to agent: reward=14.0 reward_min=0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<1936 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:35,499] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 9 reward messages to agent: reward=30.0 reward_min=1.0 reward_max=9.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:36,514] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.01s, sent 8 reward messages to agent: reward=17.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:37,702] [INFO:root] [Rewarder] Rewarder fell behind by 0.1751880645751953s from target; losing 10 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:38,597] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=8.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=277393.0 vnc_pixels_ps[total]=128320.5 reward_lag=None rewarder_message_lag=None fps=54.14\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:38,637] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.12s, sent 8 reward messages to agent: reward=21.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:38,672] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"rewarder.sleep.missed\": {\"calls\": 25, \"std\": \"37.78ms\", \"mean\": \"19.80ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 39, \"std\": \"51.65us\", \"mean\": \"110.32us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 42, \"std\": \"29.55us\", \"mean\": \"68.87us\"}, \"rewarder.sleep\": {\"calls\": 246, \"std\": \"3.88ms\", \"mean\": \"14.45ms\"}, \"rewarder.compute_reward\": {\"calls\": 271, \"std\": \"13.53ms\", \"mean\": \"4.02ms\"}, \"rewarder.frame\": {\"calls\": 271, \"std\": \"13.19ms\", \"mean\": \"19.47ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 42, \"std\": \"82.96us\", \"mean\": \"123.46us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 271, \"std\": \"2.33ms\", \"mean\": \"475.01us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 39, \"std\": \"28.38ms\", \"mean\": \"19.42ms\"}, \"reward.parsing.score\": {\"calls\": 42, \"std\": \"27.78ms\", \"mean\": \"18.56ms\"}, \"reward.parsing.gameover\": {\"calls\": 42, \"std\": \"135.39us\", \"mean\": \"292.39us\"}} counters={\"agent_conn.reward\": {\"calls\": 31, \"std\": 2.0144638284489083, \"mean\": 2.516129032258064}, \"reward.vnc.updates.n\": {\"calls\": 271, \"std\": 0.36255663622825157, \"mean\": 0.15498154981549817}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 3, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 3, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 42, \"std\": 21.56206771688826, \"value\": 14338.0, \"mean\": 14304.83333333333}} (export_time=308.51us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:39,764] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.13s, sent 4 reward messages to agent: reward=18.0 reward_min=0 reward_max=13.0 done=False info={'rewarder.vnc.updates.bytes': 36908, 'rewarder.vnc.updates.pixels': 21840, 'rewarder.profile': '<2183 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:42,045] [INFO:universe.utils] [gameover] Gameover screen detected: distance_n=0.00037073 match_time=244us\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:42,045] [INFO:gym_controlplane.reward.reward] RESET CAUSE: gameover state reached\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:42,045] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.28s, sent 5 reward messages to agent: reward=23.0 reward_min=0.0 reward_max=13.0 done=True info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:42,047] [INFO:root] [Rewarder] Resetting environment since done=True\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:42,047] [INFO:root] [Rewarder] Triggering a reset on EnvController\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:42,047] [INFO:root] [EnvStatus] Changing env_state: running (env_id=flashgames.DuskDrive-v0) -> resetting (env_id=flashgames.DuskDrive-v0) (episode_id: 3->4, fps=60)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:42,048] [INFO:root] [Rewarder] Blocking until env finishes resetting\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:42,048] [INFO:root] [EnvController] controlplane.py is resetting the environment\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:42,048] [INFO:root] [EnvController] Env state: env_id=flashgames.DuskDrive-v0 episode_id=4\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:42,050] [INFO:root] [EnvController] Writing flashgames.DuskDrive-v0 to /tmp/demo/env_id.txt\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:42,051] [ERROR:root] Closing server (via subprocess.close()) and all chromes (via pkill chromedriver || :; pkill chrome || :)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:11:42,097] init detected end of child process 929 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:11:42,131] init detected end of child process 944 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:11:42,135] init detected end of child process 1156 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:11:42,198] init detected end of child process 1169 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [Wed Jun 28 07:11:42 UTC 2017] [/usr/local/bin/sudoable-env-setup] Allowing outbound network traffic to non-private IPs for git-lfs. (Going to fetch files via git lfs.)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [Wed Jun 28 07:11:42 UTC 2017] [/usr/local/bin/sudoable-env-setup] Completion file /usr/local/openai/git-lfs/flashgames.DuskDrive-v0 exists; not git-lfs pulling\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:11:42,254] init detected end of child process 1179 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [Wed Jun 28 07:11:42 UTC 2017] [/usr/local/bin/sudoable-env-setup] Disabling outbound network traffic for flashgames.DuskDrive-v0\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:42,279] [INFO:gym_flashgames.launcher] [EnvController] Launching new Chrome process (attempt 0/10)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:42,279] [INFO:root] Replacing selenium_wrapper_server since we currently do it at every episode boundary\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:11:42,348] init detected end of child process 940 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:11:42,348] init detected end of child process 941 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:11:42,349] init detected end of child process 943 with exit code 0, killed by SIGTERM: 15\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [init] [2017-06-28 07:11:42,350] init detected end of child process 932 with exit code 0, not killed by signal\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:42,461] [selenium_wrapper_server] Calling webdriver.Chrome()\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:11:43 [info] 63#63: *44 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:11:43 [info] 63#63: *45 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:11:44 [info] 63#63: *46 client sent invalid request while reading client request line, client: 127.0.0.1, server: , request: \"CONNECT www.google.com:443 HTTP/1.1\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:44,077] [selenium_wrapper_server] Call to webdriver.Chrome() completed: 1.62s\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:44,081] [INFO:gym_flashgames.launcher] [EnvController] Navigating browser to url=http://localhost/flashgames.DuskDrive-v0\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:44,214] [INFO:root] [EnvController] Running command: /app/universe-envs/controlplane/bin/play_vexpect -e flashgames.DuskDrive-v0 -r vnc://127.0.0.1:5900 -d\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:45,388] [play_vexpect] No rewarder addresses were provided, so this env cannot connect to the remote's rewarder channel, and cannot send control messages (e.g. reset)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:45,388] [play_vexpect] Using the golang VNC implementation\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:45,388] [play_vexpect] Using VNCSession arguments: {'fine_quality_level': 50, 'start_timeout': 7, 'encoding': 'zrle', 'compress_level': 0, 'subsample_level': 2}. (Customize by running \"env.configure(vnc_kwargs={...})\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:45,390] [play_vexpect] Printed stats will ignore clock skew. (This usually makes sense only when the environment and agent are on the same machine.)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:45,396] [play_vexpect] [0] Connecting to environment: vnc://127.0.0.1:5900 password=openai. If desired, you can manually connect a VNC viewer, such as TurboVNC. Most environments provide a convenient in-browser VNC client: http://None/viewer/?password=openai\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:45,396] [play_vexpect] [0] Connecting to environment details: vnc_address=127.0.0.1:5900 vnc_password=openai rewarder_address=None rewarder_password=openai\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m 2017/06/28 07:11:45 I0628 07:11:45.397851 1677 gymvnc.go:417] [0:127.0.0.1:5900] opening connection to VNC server\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Wed Jun 28 07:11:45 2017\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  Connections: accepted: 127.0.0.1::41336\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  SConnection: Client needs protocol version 3.8\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  SConnection: Client requests security type VncAuth(2)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  VNCSConnST:  Server default pixel format depth 24 (32bpp) little-endian rgb888\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  VNCSConnST:  Client pixel format depth 24 (32bpp) little-endian bgr888\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m 2017/06/28 07:11:45 I0628 07:11:45.400345 1677 gymvnc.go:550] [0:127.0.0.1:5900] connection established\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:45,906] [play_vexpect] Waiting for any of [MaskState<initializing0>] to activate\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:52,966] [play_vexpect] Applying transition: ClickTransition<initializing0->['initializing1'] x=429 y=539 buttonmask=1> for active state MaskState<initializing0>. (Summary: plausible_states=MaskState<initializing0> distance_m=0.0489 match_time_m=306us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:52,979] [play_vexpect] Waiting for any of [MaskState<initializing1>] to activate (or whether any of [MaskState<initializing0>] are still active)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m 2017/06/28 07:11:53 I0628 07:11:53.044955 60 gymvnc.go:374] [0:127.0.0.1:5900] update queue max of 60 reached; pausing further updates\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:57,030] [play_vexpect] Advancing to the next hopeful state (2/2): MaskState<initializing0>\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:59,078] [play_vexpect] Advancing to the next hopeful state (1/2): MaskState<initializing1>\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:59,229] [play_vexpect] Applying transition: ClickTransition<initializing1->['initializing2'] x=571 y=512 buttonmask=1> for active state MaskState<initializing1>. (Summary: plausible_states=[MaskState<initializing1>, MaskState<initializing0>] distance_m=[0.0044, 0.7509666666666667] match_time_m=['265us', '295us'])\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:59,247] [play_vexpect] Waiting for any of [MaskState<initializing2>] to activate (or whether any of [MaskState<initializing1>] are still active)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:59,846] [play_vexpect] Applying transition: ClickTransition<initializing2->['ready0', 'ready1', 'ready2', 'ready3'] x=216 y=296 buttonmask=1> for active state MaskState<initializing2>. (Summary: plausible_states=[MaskState<initializing2>, MaskState<initializing1>] distance_m=[0.0359, 0.9961] match_time_m=['385us', '196us'])\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:11:59,862] [play_vexpect] Waiting for any of [ready0, ready1, ready2, ready3] to activate (or whether any of [MaskState<initializing2>] are still active)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:00,582] [play_vexpect] Applying transition: ClickTransition<ready1->[] x=0 y=0 buttonmask=0> for active state ready1. (Summary: plausible_states=[ready0, ready1, ready2, ready3, MaskState<initializing2>] distance_m=[0.31468353, 0.11631844, 0.17798968, 0.039567132, 0.9968333333333333] match_time_m=['830us', '486us', '160us', '160us', '232us'])\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:00,587] [play_vexpect] Reaching start state: ready1\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:00,588] [play_vexpect] vexpect macro complete in 15.164911s\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Wed Jun 28 07:12:00 2017\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  Connections: closed: 127.0.0.1::41336 (Clean disconnection)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager: Framebuffer updates: 206\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:   ZRLE:\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Solid: 24 rects, 854.353 kpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:            1.97266 KiB (1:1691.93 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Bitmap RLE: 79 rects, 1.2197 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:                 158.793 KiB (1:30.0101 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Indexed RLE: 126 rects, 948.869 kpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:                  286.354 KiB (1:12.949 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Full Colour: 189 rects, 7.24132 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:                  20.7273 MiB (1:1.33281 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:   Total: 418 rects, 10.2642 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:          21.1639 MiB (1:1.85031 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:01,083] [INFO:root] [EnvStatus] Changing env_state: resetting (env_id=flashgames.DuskDrive-v0) -> running (env_id=flashgames.DuskDrive-v0) (episode_id: 4->4, fps=60)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:01,087] [INFO:root] [Rewarder] Unblocking since env reset finished\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:01,088] [INFO:root] [Rewarder] Clearing reward_parser state: env_id=flashgames.DuskDrive-v0 episode_id=3->4, env_state=running\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:01,093] [INFO:universe.rewarder.remote] [Rewarder] Over past 19.05s, sent 0 reward messages to agent: reward=0 reward_min=(empty) reward_max=(empty) done=False info={}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:01,098] [INFO:universe.rewarder.remote] [Rewarder] Ending previous episode: episode_reward=13686.0 episode_count=546 episode_duration=97.68\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m 2017/06/28 07:12:01 I0628 07:12:01.103616 60 gymvnc.go:278] [0:127.0.0.1:5900] resuming updates\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:01,105] [INFO:universe.wrappers.logger] Stats for the past 22.51s: vnc_updates_ps=1.5 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=34270.8 vnc_pixels_ps[total]=19706.9 reward_lag=None rewarder_message_lag=None fps=9.02\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:01,139] [INFO:universe.pyprofile] [pyprofile] period=22.47s timers={\"rewarder.sleep.missed\": {\"calls\": 11, \"std\": \"6.06ms\", \"mean\": \"5.03ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 21, \"std\": \"81.69us\", \"mean\": \"122.72us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 33, \"std\": \"41.08us\", \"mean\": \"76.17us\"}, \"rewarder.sleep\": {\"calls\": 187, \"std\": \"2.77ms\", \"mean\": \"15.13ms\"}, \"rewarder.compute_reward\": {\"calls\": 199, \"std\": \"6.16ms\", \"mean\": \"2.50ms\"}, \"rewarder.frame\": {\"calls\": 199, \"std\": \"1.81ms\", \"mean\": \"17.27ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 33, \"std\": \"130.99us\", \"mean\": \"138.39us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 199, \"std\": \"294.13us\", \"mean\": \"171.43us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 21, \"std\": \"6.03ms\", \"mean\": \"17.15ms\"}, \"reward.parsing.score\": {\"calls\": 33, \"std\": \"9.69ms\", \"mean\": \"11.47ms\"}, \"reward.parsing.gameover\": {\"calls\": 33, \"std\": \"194.71us\", \"mean\": \"282.64us\"}} counters={\"agent_conn.done\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}, \"agent_conn.reward\": {\"calls\": 9, \"std\": 5.410894360249309, \"mean\": 4.555555555555555}, \"reward.vnc.updates.n\": {\"calls\": 199, \"std\": 4.328399107119885, \"mean\": 0.4673366834170855}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 12, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 12, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 32, \"std\": 12.0990535486951, \"value\": 14381.0, \"mean\": 14376.0}} (export_time=131.61us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:01,164] [INFO:gym_controlplane.reward.reward] First score parsed: score=463\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:02,198] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.10s, sent 9 reward messages to agent: reward=1237.0 reward_min=-464.0 reward_max=464 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<2276 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:03,227] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 5 reward messages to agent: reward=1531.0 reward_min=206.0 reward_max=425.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:04,287] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 6 reward messages to agent: reward=1811.0 reward_min=34.0 reward_max=630.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:05,411] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.12s, sent 6 reward messages to agent: reward=30.0 reward_min=1.0 reward_max=13.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:06,107] [INFO:universe.wrappers.logger] Stats for the past 5.00s: vnc_updates_ps=19.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=263718.7 vnc_pixels_ps[total]=215060.9 reward_lag=None rewarder_message_lag=None fps=57.79\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:06,155] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"rewarder.sleep.missed\": {\"calls\": 32, \"std\": \"6.04ms\", \"mean\": \"6.20ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 33, \"std\": \"76.22us\", \"mean\": \"131.84us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 37, \"std\": \"164.17us\", \"mean\": \"109.22us\"}, \"rewarder.sleep\": {\"calls\": 260, \"std\": \"1.43ms\", \"mean\": \"15.67ms\"}, \"rewarder.compute_reward\": {\"calls\": 292, \"std\": \"6.44ms\", \"mean\": \"2.72ms\"}, \"rewarder.frame\": {\"calls\": 292, \"std\": \"2.73ms\", \"mean\": \"17.67ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 37, \"std\": \"79.69us\", \"mean\": \"130.55us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 292, \"std\": \"94.45us\", \"mean\": \"151.68us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 33, \"std\": \"2.79ms\", \"mean\": \"18.94ms\"}, \"reward.parsing.score\": {\"calls\": 37, \"std\": \"6.53ms\", \"mean\": \"17.45ms\"}, \"reward.parsing.gameover\": {\"calls\": 37, \"std\": \"252.21us\", \"mean\": \"379.26us\"}} counters={\"agent_conn.reward\": {\"calls\": 31, \"std\": 216.76744389133438, \"mean\": 149.5161290322581}, \"reward.vnc.updates.n\": {\"calls\": 292, \"std\": 0.3332215130430441, \"mean\": 0.12671232876712335}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 4, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 4, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 36, \"std\": 1839.7996550263074, \"value\": 5097.0, \"mean\": 3413.472222222222}} (export_time=210.05us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:06,499] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.09s, sent 9 reward messages to agent: reward=35.0 reward_min=0 reward_max=13.0 done=False info={'rewarder.vnc.updates.bytes': 33721, 'rewarder.vnc.updates.pixels': 11220, 'rewarder.profile': '<2199 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:07,598] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.10s, sent 6 reward messages to agent: reward=12.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 36908, 'rewarder.vnc.updates.pixels': 21840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:08,677] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.08s, sent 5 reward messages to agent: reward=25.0 reward_min=2.0 reward_max=8.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:10,970] [INFO:universe.rewarder.remote] [Rewarder] Over past 2.29s, sent 3 reward messages to agent: reward=13.0 reward_min=1.0 reward_max=9.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:11,124] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=5.0 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=177160.5 vnc_pixels_ps[total]=81485.2 reward_lag=None rewarder_message_lag=None fps=53.24\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:11,186] [INFO:universe.pyprofile] [pyprofile] period=5.03s timers={\"rewarder.sleep.missed\": {\"calls\": 35, \"std\": \"13.43ms\", \"mean\": \"16.17ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 26, \"std\": \"165.48us\", \"mean\": \"150.56us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 26, \"std\": \"28.37us\", \"mean\": \"74.17us\"}, \"rewarder.sleep\": {\"calls\": 231, \"std\": \"1.55ms\", \"mean\": \"15.52ms\"}, \"rewarder.compute_reward\": {\"calls\": 266, \"std\": \"10.79ms\", \"mean\": \"4.06ms\"}, \"rewarder.frame\": {\"calls\": 266, \"std\": \"8.45ms\", \"mean\": \"19.94ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 26, \"std\": \"331.51us\", \"mean\": \"195.20us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 266, \"std\": \"1.23ms\", \"mean\": \"343.80us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 26, \"std\": \"14.33ms\", \"mean\": \"30.58ms\"}, \"reward.parsing.score\": {\"calls\": 26, \"std\": \"14.27ms\", \"mean\": \"31.23ms\"}, \"reward.parsing.gameover\": {\"calls\": 26, \"std\": \"231.64us\", \"mean\": \"369.70us\"}} counters={\"agent_conn.reward\": {\"calls\": 18, \"std\": 2.6525360906910014, \"mean\": 3.2777777777777777}, \"reward.vnc.updates.n\": {\"calls\": 266, \"std\": 0.2975284772578942, \"mean\": 0.09774436090225565}} gauges={\"reward_parser.score.last_score\": {\"calls\": 26, \"std\": 20.76772644124689, \"value\": 5157.0, \"mean\": 5126.461538461538}} (export_time=142.81us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:12,195] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.22s, sent 6 reward messages to agent: reward=42.0 reward_min=1.0 reward_max=10.0 done=False info={'rewarder.vnc.updates.bytes': 31277, 'rewarder.vnc.updates.pixels': 19300, 'rewarder.profile': '<1933 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:13,256] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 5 reward messages to agent: reward=22.0 reward_min=1.0 reward_max=9.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:14,506] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.25s, sent 7 reward messages to agent: reward=20.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:15,542] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 7 reward messages to agent: reward=27.0 reward_min=1.0 reward_max=8.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:16,127] [INFO:universe.wrappers.logger] Stats for the past 5.00s: vnc_updates_ps=7.4 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=252435.5 vnc_pixels_ps[total]=119582.5 reward_lag=None rewarder_message_lag=None fps=57.59\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:16,213] [INFO:universe.pyprofile] [pyprofile] period=5.03s timers={\"rewarder.sleep.missed\": {\"calls\": 32, \"std\": \"4.97ms\", \"mean\": \"6.38ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 35, \"std\": \"143.87us\", \"mean\": \"156.40us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 37, \"std\": \"61.28us\", \"mean\": \"91.95us\"}, \"rewarder.sleep\": {\"calls\": 258, \"std\": \"1.62ms\", \"mean\": \"15.63ms\"}, \"rewarder.compute_reward\": {\"calls\": 290, \"std\": \"7.02ms\", \"mean\": \"2.95ms\"}, \"rewarder.frame\": {\"calls\": 290, \"std\": \"2.52ms\", \"mean\": \"17.70ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 37, \"std\": \"118.05us\", \"mean\": \"117.19us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 290, \"std\": \"92.76us\", \"mean\": \"144.54us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 35, \"std\": \"4.94ms\", \"mean\": \"19.92ms\"}, \"reward.parsing.score\": {\"calls\": 37, \"std\": \"6.56ms\", \"mean\": \"19.38ms\"}, \"reward.parsing.gameover\": {\"calls\": 37, \"std\": \"256.59us\", \"mean\": \"402.24us\"}} counters={\"agent_conn.reward\": {\"calls\": 29, \"std\": 3.0718008710092204, \"mean\": 4.310344827586207}, \"reward.vnc.updates.n\": {\"calls\": 290, \"std\": 0.33420519951075894, \"mean\": 0.12758620689655176}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 2, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 2, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 37, \"std\": 32.72099729017581, \"value\": 5282.0, \"mean\": 5233.945945945946}} (export_time=168.56us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:16,680] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.14s, sent 7 reward messages to agent: reward=17.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<2190 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:17,698] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.02s, sent 12 reward messages to agent: reward=29.0 reward_min=1.0 reward_max=8.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:18,738] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 9 reward messages to agent: reward=19.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 36908, 'rewarder.vnc.updates.pixels': 21840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:20,047] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.31s, sent 5 reward messages to agent: reward=13.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:20,048] [INFO:root] [Rewarder] Rewarder fell behind by 0.12108802795410156s from target; losing 7 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:20,344] [INFO:root] [Rewarder] Rewarder fell behind by 0.1454920768737793s from target; losing 8 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:21,130] [INFO:universe.wrappers.logger] Stats for the past 5.00s: vnc_updates_ps=7.8 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=273248.7 vnc_pixels_ps[total]=120898.1 reward_lag=None rewarder_message_lag=None fps=53.77\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:21,213] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"rewarder.sleep.missed\": {\"calls\": 37, \"std\": \"29.65ms\", \"mean\": \"14.01ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 37, \"std\": \"141.54us\", \"mean\": \"147.03us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 38, \"std\": \"83.19us\", \"mean\": \"86.92us\"}, \"rewarder.sleep\": {\"calls\": 233, \"std\": \"3.53ms\", \"mean\": \"14.66ms\"}, \"rewarder.compute_reward\": {\"calls\": 270, \"std\": \"10.59ms\", \"mean\": \"3.90ms\"}, \"rewarder.frame\": {\"calls\": 270, \"std\": \"15.27ms\", \"mean\": \"20.16ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 38, \"std\": \"1.07ms\", \"mean\": \"339.53us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 270, \"std\": \"1.97ms\", \"mean\": \"492.43us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 37, \"std\": \"14.95ms\", \"mean\": \"19.47ms\"}, \"reward.parsing.score\": {\"calls\": 38, \"std\": \"16.05ms\", \"mean\": \"19.80ms\"}, \"reward.parsing.gameover\": {\"calls\": 38, \"std\": \"426.91us\", \"mean\": \"452.46us\"}} counters={\"agent_conn.reward\": {\"calls\": 29, \"std\": 1.8003831009940154, \"mean\": 2.2068965517241383}, \"reward.vnc.updates.n\": {\"calls\": 270, \"std\": 0.34839969703150214, \"mean\": 0.14074074074074075}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 38, \"std\": 16.85415642815348, \"value\": 5345.0, \"mean\": 5315.210526315791}} (export_time=170.71us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:21,214] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.17s, sent 1 reward messages to agent: reward=0 reward_min=0 reward_max=0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.vnc.updates.pixels': 0, 'rewarder.profile': '<2144 bytes>', 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:22,689] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.47s, sent 1 reward messages to agent: reward=1.0 reward_min=1.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:23,982] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.29s, sent 7 reward messages to agent: reward=63.0 reward_min=1.0 reward_max=20.0 done=False info={'rewarder.vnc.updates.bytes': 33721, 'rewarder.vnc.updates.pixels': 11220, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:25,016] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 14 reward messages to agent: reward=107.0 reward_min=1.0 reward_max=24.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:26,043] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.03s, sent 12 reward messages to agent: reward=804.0 reward_min=23.0 reward_max=128.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:26,133] [INFO:universe.wrappers.logger] Stats for the past 5.00s: vnc_updates_ps=8.2 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=286614.3 vnc_pixels_ps[total]=134613.5 reward_lag=None rewarder_message_lag=None fps=56.78\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:26,235] [INFO:universe.pyprofile] [pyprofile] period=5.02s timers={\"rewarder.sleep.missed\": {\"calls\": 28, \"std\": \"15.08ms\", \"mean\": \"9.81ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 42, \"std\": \"51.09us\", \"mean\": \"92.28us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 42, \"std\": \"88.89us\", \"mean\": \"72.89us\"}, \"rewarder.sleep\": {\"calls\": 256, \"std\": \"3.32ms\", \"mean\": \"14.84ms\"}, \"rewarder.compute_reward\": {\"calls\": 284, \"std\": \"6.48ms\", \"mean\": \"2.94ms\"}, \"rewarder.frame\": {\"calls\": 284, \"std\": \"8.56ms\", \"mean\": \"18.84ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 42, \"std\": \"92.82us\", \"mean\": \"109.58us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 284, \"std\": \"144.04us\", \"mean\": \"140.20us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 42, \"std\": \"5.92ms\", \"mean\": \"16.24ms\"}, \"reward.parsing.score\": {\"calls\": 42, \"std\": \"5.90ms\", \"mean\": \"16.72ms\"}, \"reward.parsing.gameover\": {\"calls\": 42, \"std\": \"181.08us\", \"mean\": \"275.25us\"}} counters={\"agent_conn.reward\": {\"calls\": 36, \"std\": 35.115444528646435, \"mean\": 27.861111111111107}, \"reward.vnc.updates.n\": {\"calls\": 284, \"std\": 0.36878866088149237, \"mean\": 0.15140845070422526}} gauges={\"reward_parser.score.last_score\": {\"calls\": 42, \"std\": 296.6444911389929, \"value\": 6349.0, \"mean\": 5576.404761904764}} (export_time=98.23us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:27,087] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 8 reward messages to agent: reward=74.0 reward_min=1.0 reward_max=28.0 done=False info={'rewarder.vnc.updates.bytes': 344, 'rewarder.vnc.updates.pixels': 2400, 'rewarder.profile': '<1933 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:28,129] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 10 reward messages to agent: reward=19.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:29,130] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.00s, sent 10 reward messages to agent: reward=21.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:30,193] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 9 reward messages to agent: reward=19.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:31,149] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=12.6 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=435946.9 vnc_pixels_ps[total]=197405.6 reward_lag=None rewarder_message_lag=None fps=58.03\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:31,237] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"rewarder.sleep.missed\": {\"calls\": 28, \"std\": \"4.39ms\", \"mean\": \"5.86ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 60, \"std\": \"82.56us\", \"mean\": \"106.58us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 62, \"std\": \"45.16us\", \"mean\": \"70.74us\"}, \"rewarder.sleep\": {\"calls\": 263, \"std\": \"4.32ms\", \"mean\": \"14.19ms\"}, \"rewarder.compute_reward\": {\"calls\": 291, \"std\": \"7.27ms\", \"mean\": \"3.87ms\"}, \"rewarder.frame\": {\"calls\": 291, \"std\": \"2.14ms\", \"mean\": \"17.59ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 62, \"std\": \"72.27us\", \"mean\": \"111.00us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 291, \"std\": \"94.46us\", \"mean\": \"139.12us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 60, \"std\": \"5.33ms\", \"mean\": \"15.97ms\"}, \"reward.parsing.score\": {\"calls\": 62, \"std\": \"6.09ms\", \"mean\": \"15.96ms\"}, \"reward.parsing.gameover\": {\"calls\": 62, \"std\": \"180.99us\", \"mean\": \"289.27us\"}} counters={\"agent_conn.reward\": {\"calls\": 45, \"std\": 2.920149435777684, \"mean\": 2.8000000000000007}, \"reward.vnc.updates.n\": {\"calls\": 291, \"std\": 0.4101739664853441, \"mean\": 0.2130584192439863}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 2, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 2, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 62, \"std\": 28.209947951132172, \"value\": 6474.0, \"mean\": 6428.741935483871}} (export_time=241.28us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:31,238] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.04s, sent 10 reward messages to agent: reward=21.0 reward_min=0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.vnc.updates.pixels': 0, 'rewarder.profile': '<2194 bytes>', 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:32,402] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.16s, sent 8 reward messages to agent: reward=20.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:33,487] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.08s, sent 11 reward messages to agent: reward=26.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:34,543] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 12 reward messages to agent: reward=20.0 reward_min=1.0 reward_max=5.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:35,607] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.06s, sent 8 reward messages to agent: reward=18.0 reward_min=1.0 reward_max=4.0 done=False info={'rewarder.vnc.updates.bytes': 19695, 'rewarder.vnc.updates.pixels': 6552, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:36,156] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=13.2 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=438748.1 vnc_pixels_ps[total]=194555.7 reward_lag=None rewarder_message_lag=None fps=54.74\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:36,251] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"rewarder.sleep.missed\": {\"calls\": 41, \"std\": \"16.87ms\", \"mean\": \"10.57ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 62, \"std\": \"91.68us\", \"mean\": \"133.35us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 65, \"std\": \"273.61us\", \"mean\": \"110.84us\"}, \"rewarder.sleep\": {\"calls\": 232, \"std\": \"4.68ms\", \"mean\": \"13.75ms\"}, \"rewarder.compute_reward\": {\"calls\": 273, \"std\": \"9.97ms\", \"mean\": \"4.96ms\"}, \"rewarder.frame\": {\"calls\": 273, \"std\": \"8.70ms\", \"mean\": \"19.62ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 65, \"std\": \"198.74us\", \"mean\": \"142.15us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 273, \"std\": \"2.37ms\", \"mean\": \"313.09us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 61, \"std\": \"12.65ms\", \"mean\": \"16.43ms\"}, \"reward.parsing.score\": {\"calls\": 65, \"std\": \"12.88ms\", \"mean\": \"16.01ms\"}, \"reward.parsing.gameover\": {\"calls\": 65, \"std\": \"411.51us\", \"mean\": \"399.96us\"}} counters={\"agent_conn.reward\": {\"calls\": 40, \"std\": 1.3922864426767714, \"mean\": 2.1000000000000005}, \"reward.vnc.updates.n\": {\"calls\": 273, \"std\": 0.42669992867547174, \"mean\": 0.23809523809523803}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 3, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 4, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 65, \"std\": 22.77246355039059, \"value\": 6558.0, \"mean\": 6520.90769230769}} (export_time=325.44us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:37,258] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.65s, sent 2 reward messages to agent: reward=15.0 reward_min=0 reward_max=15.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.profile': '<2187 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:38,425] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.17s, sent 5 reward messages to agent: reward=28.0 reward_min=1.0 reward_max=18.0 done=False info={'rewarder.vnc.updates.bytes': 35041, 'rewarder.vnc.updates.pixels': 21020, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:39,498] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.07s, sent 9 reward messages to agent: reward=19.0 reward_min=1.0 reward_max=6.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:41,173] [INFO:universe.wrappers.logger] Stats for the past 5.00s: vnc_updates_ps=7.4 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=255878.4 vnc_pixels_ps[total]=118891.6 reward_lag=None rewarder_message_lag=None fps=56.37\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:41,286] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"rewarder.sleep.missed\": {\"calls\": 22, \"std\": \"19.40ms\", \"mean\": \"13.93ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 36, \"std\": \"24.58us\", \"mean\": \"90.82us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 37, \"std\": \"30.87us\", \"mean\": \"64.75us\"}, \"rewarder.sleep\": {\"calls\": 261, \"std\": \"3.60ms\", \"mean\": \"14.39ms\"}, \"rewarder.compute_reward\": {\"calls\": 283, \"std\": \"8.88ms\", \"mean\": \"3.34ms\"}, \"rewarder.frame\": {\"calls\": 283, \"std\": \"6.76ms\", \"mean\": \"18.49ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 37, \"std\": \"35.75us\", \"mean\": \"89.61us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 283, \"std\": \"1.24ms\", \"mean\": \"316.73us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 36, \"std\": \"13.64ms\", \"mean\": \"16.67ms\"}, \"reward.parsing.score\": {\"calls\": 37, \"std\": \"13.77ms\", \"mean\": \"16.64ms\"}, \"reward.parsing.gameover\": {\"calls\": 37, \"std\": \"81.25us\", \"mean\": \"256.08us\"}} counters={\"agent_conn.reward\": {\"calls\": 25, \"std\": 4.319336368780123, \"mean\": 3.3600000000000003}, \"reward.vnc.updates.n\": {\"calls\": 283, \"std\": 0.337715227772666, \"mean\": 0.1307420494699647}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 1, \"std\": 0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 37, \"std\": 24.088363357630552, \"value\": 6643.0, \"mean\": 6606.0270270270275}} (export_time=31.67ms)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:41,303] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.80s, sent 10 reward messages to agent: reward=22.0 reward_min=0 reward_max=7.0 done=False info={'rewarder.vnc.updates.bytes': 0, 'rewarder.vnc.updates.pixels': 0, 'rewarder.profile': '<2144 bytes>', 'rewarder.vnc.updates.n': 0}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:12:41,586] [INFO:root] [Rewarder] Rewarder fell behind by 0.3274521827697754s from target; losing 19 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-28 03:14:19,024] Throttle fell behind by 96.96s; lost 5817.41 frames\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:19 [info] 63#63: *48 client timed out (110: Connection timed out) while waiting for request, client: 127.0.0.1, server: 0.0.0.0:80\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:19 [info] 64#64: *10 upstream timed out (110: Connection timed out) while proxying upgraded connection, client: 172.17.0.1, server: , request: \"GET / HTTP/1.1\", upstream: \"http://127.0.0.1:15901/\", host: \"localhost:15901\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-28 03:14:19,272] [0] Closing rewarder connection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:19 [info] 63#63: *49 client timed out (110: Connection timed out) while waiting for request, client: 127.0.0.1, server: 0.0.0.0:80\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:19 [info] 63#63: *50 client timed out (110: Connection timed out) while waiting for request, client: 127.0.0.1, server: 0.0.0.0:80\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:19 [info] 63#63: *51 client timed out (110: Connection timed out) while waiting for request, client: 127.0.0.1, server: 0.0.0.0:80\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 172.17.0.1 - openai [28/Jun/2017:07:14:19 +0000] \"GET / HTTP/1.1\" 101 415820 \"-\" \"AutobahnPython/17.6.2\"\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] \n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc] Wed Jun 28 07:14:19 2017\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  Connections: closed: 172.17.0.1::35300 (Clean disconnection)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager: Framebuffer updates: 2660\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:   Tight:\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Solid: 242 rects, 11.4382 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:            3.78125 KiB (1:11817.1 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Bitmap RLE: 1.18 krects, 14.9321 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:                 186.066 KiB (1:313.557 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Indexed RLE: 510 rects, 4.55045 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:                  374.857 KiB (1:47.4345 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:   Tight (JPEG):\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:     Full Colour: 14.615 krects, 805.933 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:                  75.0751 MiB (1:40.9531 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:   Total: 16.547 krects, 836.854 Mpixels\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [tigervnc]  EncodeManager:          75.6266 MiB (1:42.2145 ratio)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:19,591] [INFO:universe.rewarder.remote] WebSocket connection closed: connection was closed uncleanly (peer dropped the TCP connection without previous WebSocket closing handshake)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:19,593] [INFO:universe.rewarder.remote] [Twisted] Active client disconnected (sent 11 messages). Still have 0 active clients left\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:19,594] [INFO:universe.wrappers.logger] Stats for the past 98.42s: vnc_updates_ps=0.0 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=0.0 vnc_pixels_ps[total]=0.0 reward_lag=None rewarder_message_lag=None fps=0.36\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:19,671] [INFO:universe.pyprofile] [pyprofile] period=98.42s timers={\"rewarder.sleep.missed\": {\"calls\": 1, \"std\": \"0.00us\", \"mean\": \"327.45ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 1, \"std\": \"0.00us\", \"mean\": \"191.21us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 1, \"std\": \"0.00us\", \"mean\": \"85.59us\"}, \"rewarder.sleep\": {\"calls\": 29, \"std\": \"992.63us\", \"mean\": \"15.37ms\"}, \"rewarder.compute_reward\": {\"calls\": 30, \"std\": \"15.83ms\", \"mean\": \"3.59ms\"}, \"rewarder.frame\": {\"calls\": 30, \"std\": \"17.80s\", \"mean\": \"3.28s\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 1, \"std\": \"0.00us\", \"mean\": \"9.90ms\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 30, \"std\": \"303.16us\", \"mean\": \"255.02us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 1, \"std\": \"0.00us\", \"mean\": \"52.59ms\"}, \"reward.parsing.score\": {\"calls\": 1, \"std\": \"0.00us\", \"mean\": \"65.93ms\"}, \"reward.parsing.gameover\": {\"calls\": 1, \"std\": \"0.00us\", \"mean\": \"520.71us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"std\": 0, \"mean\": 0.0}, \"reward.vnc.updates.n\": {\"calls\": 30, \"std\": 0.18257418583505536, \"mean\": 0.03333333333333333}} gauges={\"reward_parser.score.last_score\": {\"calls\": 1, \"std\": 0, \"value\": 6643.0, \"mean\": 6643.0}} (export_time=180.96us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:19,673] [INFO:universe.rewarder.remote] [Rewarder] Over past 98.32s, sent 1 reward messages to agent: reward=0.0 reward_min=0.0 reward_max=0.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<1685 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:19,674] [INFO:root] [Rewarder] Rewarder fell behind by 97.58465528488159s from target; losing 5855 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-28 03:14:19,703] [0:localhost:5901] Could not recalibrate network: [Failure instance: Traceback (failure with no frames): <class 'universe.error.Error'>: Can't send message to closed connection\n",
      "]\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "1/1 environments have crashed! Most recent error: {'0': 'Rewarder session failed: Lost connection: connection was closed uncleanly (peer dropped the TCP connection without previous WebSocket closing handshake) (clean=False code=1006)'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-055343b60cba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-0f29313bb688>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;31m# Save new variables for each iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anmol.j/Personal/Talks/OReillyAI-Gamebots/gamebots/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anmol.j/Personal/Talks/OReillyAI-Gamebots/gamebots/lib/python2.7/site-packages/universe/wrappers/timer.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpyprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vnc_env.Timer.step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Calculate how much time was spent actually doing work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anmol.j/Personal/Talks/OReillyAI-Gamebots/gamebots/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anmol.j/Personal/Talks/OReillyAI-Gamebots/gamebots/lib/python2.7/site-packages/universe/wrappers/render.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anmol.j/Personal/Talks/OReillyAI-Gamebots/gamebots/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anmol.j/Personal/Talks/OReillyAI-Gamebots/gamebots/lib/python2.7/site-packages/universe/wrappers/throttle.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0maccum_observation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccum_reward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccum_done_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccum_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_substep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0maccum_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'throttle.action.available_at'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anmol.j/Personal/Talks/OReillyAI-Gamebots/gamebots/lib/python2.7/site-packages/universe/wrappers/throttle.pyc\u001b[0m in \u001b[0;36m_substep\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# Submit the action ASAP, before the thread goes to sleep.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mavailable_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'throttle.observation.available_at'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anmol.j/Personal/Talks/OReillyAI-Gamebots/gamebots/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anmol.j/Personal/Talks/OReillyAI-Gamebots/gamebots/lib/python2.7/site-packages/universe/envs/vnc_env.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action_n)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_initial_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_err_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvnc_err_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_crashed_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minfo_n\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anmol.j/Personal/Talks/OReillyAI-Gamebots/gamebots/lib/python2.7/site-packages/universe/envs/vnc_env.pyc\u001b[0m in \u001b[0;36m_handle_crashed_n\u001b[0;34m(self, info_n)\u001b[0m\n\u001b[1;32m    535\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{} environments have crashed. No error key in info_n: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrashed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{} environments have crashed! Most recent error: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrashed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_propagate_obs_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_info_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: 1/1 environments have crashed! Most recent error: {'0': 'Rewarder session failed: Lost connection: connection was closed uncleanly (peer dropped the TCP connection without previous WebSocket closing handshake) (clean=False code=1006)'}"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:19,911] [INFO:root] [Rewarder] Rewarder fell behind by 0.1510481834411621s from target; losing 9 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:20 [info] 61#61: *55 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:20 [info] 61#61: *56 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:20 [info] 61#61: *57 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:20 [info] 61#61: *58 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:20 [info] 61#61: *59 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:20 [info] 61#61: *60 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:20 [info] 61#61: *61 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:20 [info] 61#61: *62 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:20 [info] 61#61: *63 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [nginx] 2017/06/28 07:14:20 [info] 61#61: *64 client closed connection while waiting for request, client: 172.17.0.1, server: 0.0.0.0:15900\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:20,062] [INFO:root] [Rewarder] Rewarder fell behind by 0.11765599250793457s from target; losing 7 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:21,630] [INFO:root] [Rewarder] Rewarder fell behind by 0.3191256523132324s from target; losing 19 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:23,527] [INFO:universe.rewarder.remote] [Rewarder] Over past 3.85s, sent 1 reward messages to agent: reward=1.0 reward_min=1.0 reward_max=1.0 done=False info={'rewarder.vnc.updates.bytes': 36882, 'rewarder.vnc.updates.pixels': 21640, 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:24,606] [INFO:universe.wrappers.logger] Stats for the past 5.00s: vnc_updates_ps=10.0 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=285473.2 vnc_pixels_ps[total]=109117.9 reward_lag=None rewarder_message_lag=None fps=45.38\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:24,672] [INFO:universe.pyprofile] [pyprofile] period=5.00s timers={\"rewarder.sleep.missed\": {\"calls\": 32, \"std\": \"17.24s\", \"mean\": \"3.08s\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 42, \"std\": \"30.82us\", \"mean\": \"114.25us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 46, \"std\": \"51.68us\", \"mean\": \"75.02us\"}, \"rewarder.sleep\": {\"calls\": 199, \"std\": \"2.61ms\", \"mean\": \"14.61ms\"}, \"rewarder.compute_reward\": {\"calls\": 231, \"std\": \"13.56ms\", \"mean\": \"3.98ms\"}, \"rewarder.frame\": {\"calls\": 231, \"std\": \"6.42s\", \"mean\": \"447.26ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 46, \"std\": \"1.40ms\", \"mean\": \"341.33us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 231, \"std\": \"2.00ms\", \"mean\": \"474.22us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 11, \"std\": \"11.87ms\", \"mean\": \"20.73ms\"}, \"reward.parsing.score\": {\"calls\": 46, \"std\": \"10.56ms\", \"mean\": \"5.60ms\"}, \"reward.parsing.gameover\": {\"calls\": 46, \"std\": \"99.53us\", \"mean\": \"345.78us\"}} counters={\"agent_conn.reward\": {\"calls\": 4, \"std\": 29.011491975882016, \"mean\": 15.5}, \"reward.vnc.updates.n\": {\"calls\": 231, \"std\": 0.4768299865386909, \"mean\": 0.2251082251082251}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 4, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 35, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 46, \"std\": 19.359652250639883, \"value\": 6705.0, \"mean\": 6698.217391304349}} (export_time=145.44us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:24,674] [INFO:universe.rewarder.remote] [Rewarder] Over past 1.15s, sent 3 reward messages to agent: reward=61.0 reward_min=0.0 reward_max=59.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<2164 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:26,414] [INFO:root] [Rewarder] Rewarder fell behind by 0.1923811435699463s from target; losing 11 frames\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:29,618] [INFO:universe.wrappers.logger] Stats for the past 5.01s: vnc_updates_ps=41.1 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=629313.1 vnc_pixels_ps[total]=250340.0 reward_lag=None rewarder_message_lag=None fps=54.30\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:29,698] [INFO:universe.pyprofile] [pyprofile] period=5.03s timers={\"rewarder.sleep.missed\": {\"calls\": 19, \"std\": \"46.78ms\", \"mean\": \"25.60ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 72, \"std\": \"49.36us\", \"mean\": \"113.30us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 185, \"std\": \"101.36us\", \"mean\": \"101.16us\"}, \"rewarder.sleep\": {\"calls\": 252, \"std\": \"4.07ms\", \"mean\": \"13.44ms\"}, \"rewarder.compute_reward\": {\"calls\": 271, \"std\": \"9.72ms\", \"mean\": \"4.17ms\"}, \"rewarder.frame\": {\"calls\": 271, \"std\": \"16.15ms\", \"mean\": \"20.07ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 185, \"std\": \"118.76us\", \"mean\": \"128.58us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 271, \"std\": \"3.84ms\", \"mean\": \"445.71us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 45, \"std\": \"14.55ms\", \"mean\": \"16.88ms\"}, \"reward.parsing.score\": {\"calls\": 185, \"std\": \"10.21ms\", \"mean\": \"4.60ms\"}, \"reward.parsing.gameover\": {\"calls\": 185, \"std\": \"193.63us\", \"mean\": \"284.35us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"std\": 0, \"mean\": 0.0}, \"reward.vnc.updates.n\": {\"calls\": 271, \"std\": 0.577113504664931, \"mean\": 0.7564575645756454}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 113, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 140, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 185, \"std\": 0.0, \"value\": 6705.0, \"mean\": 6705.0}} (export_time=327.35us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:29,700] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.03s, sent 1 reward messages to agent: reward=0.0 reward_min=0.0 reward_max=0.0 done=False info={'rewarder.vnc.updates.bytes': 35562, 'rewarder.vnc.updates.pixels': 11840, 'rewarder.profile': '<2121 bytes>', 'rewarder.vnc.updates.n': 1}\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:34,636] [INFO:universe.wrappers.logger] Stats for the past 5.02s: vnc_updates_ps=41.5 n=1 reaction_time=None observation_lag=None action_lag=None reward_ps=0.0 reward_total=0.0 vnc_bytes_ps[total]=469488.8 vnc_pixels_ps[total]=197970.0 reward_lag=None rewarder_message_lag=None fps=56.61\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:34,710] [INFO:universe.pyprofile] [pyprofile] period=5.01s timers={\"rewarder.sleep.missed\": {\"calls\": 25, \"std\": \"18.18ms\", \"mean\": \"11.00ms\"}, \"score.crop_cache.readthrough.MatchImage\": {\"calls\": 39, \"std\": \"48.20us\", \"mean\": \"107.22us\"}, \"score.crop_cache.get.MatchImage\": {\"calls\": 200, \"std\": \"87.07us\", \"mean\": \"107.85us\"}, \"rewarder.sleep\": {\"calls\": 259, \"std\": \"3.21ms\", \"mean\": \"14.32ms\"}, \"rewarder.compute_reward\": {\"calls\": 284, \"std\": \"9.33ms\", \"mean\": \"3.87ms\"}, \"rewarder.frame\": {\"calls\": 284, \"std\": \"6.18ms\", \"mean\": \"18.23ms\"}, \"score.crop_cache.get.OCRScorerV0\": {\"calls\": 200, \"std\": \"98.13us\", \"mean\": \"135.69us\"}, \"vnc_env.VNCEnv.vnc_session.step\": {\"calls\": 284, \"std\": \"748.78us\", \"mean\": \"214.90us\"}, \"score.crop_cache.readthrough.OCRScorerV0\": {\"calls\": 39, \"std\": \"13.74ms\", \"mean\": \"20.06ms\"}, \"reward.parsing.score\": {\"calls\": 200, \"std\": \"10.44ms\", \"mean\": \"4.42ms\"}, \"reward.parsing.gameover\": {\"calls\": 200, \"std\": \"911.60us\", \"mean\": \"317.88us\"}} counters={\"agent_conn.reward\": {\"calls\": 1, \"std\": 0, \"mean\": 0.0}, \"reward.vnc.updates.n\": {\"calls\": 284, \"std\": 0.5032121970936686, \"mean\": 0.7323943661971833}, \"score.crop_cache.hit.MatchImage\": {\"calls\": 161, \"std\": 0.0, \"mean\": 1.0}, \"score.crop_cache.hit.OCRScorerV0\": {\"calls\": 161, \"std\": 0.0, \"mean\": 1.0}} gauges={\"reward_parser.score.last_score\": {\"calls\": 200, \"std\": 0.0, \"value\": 6705.0, \"mean\": 6705.0}} (export_time=134.94us)\n",
      "\u001b[36muniverse-FwMjDj-0 |\u001b[0m [2017-06-28 07:14:34,712] [INFO:universe.rewarder.remote] [Rewarder] Over past 5.01s, sent 1 reward messages to agent: reward=0.0 reward_min=0.0 reward_max=0.0 done=False info={'rewarder.vnc.updates.bytes': 36923, 'rewarder.vnc.updates.pixels': 21768, 'rewarder.profile': '<2124 bytes>', 'rewarder.vnc.updates.n': 1}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\tmain()\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
